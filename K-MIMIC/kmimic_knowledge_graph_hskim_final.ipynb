{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d01b8165",
   "metadata": {},
   "source": [
    "# K‑MIMIC Knowledge Graph Demo (LLM‑assisted)\n",
    "**Last updated:** 2025-07-29\n",
    "\n",
    "This notebook demonstrates how to construct a knowledge graph (KG) from **K‑MIMIC** data using an **LLM‑assisted pipeline**. \n",
    "\n",
    "\n",
    "## What you will do\n",
    "1. **Set up** the environment and load configurations.  \n",
    "2. **Load and preprocess** K‑MIMIC data needed for the demo.  \n",
    "3. **Extract entities/relations** from text using an LLM (prompt‑based).  \n",
    "4. **Construct a knowledge graph** (in memory or Neo4j) from extracted triplets.  \n",
    "5. **Visualize and/or export** the resulting graph and artifacts.  \n",
    "\n",
    "## Requirements (minimal)\n",
    "- Python 3.9+\n",
    "- Common data libraries: `pandas`, `numpy`, `tqdm`\n",
    "- Optional graph tools: `networkx`, `pyvis` (for HTML visualization)\n",
    "- Optional LLM: `transformers`, `pytorch`, `bitsandbytes`, `sentencepeice` (for HuggingFace Open LLM models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b53023",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [0. Notebook Settings](#0-notebook-settings)\n",
    "- [1. Setup & Imports](#1-setup--imports)\n",
    "- [2. Data Loading](#2-data-loading)\n",
    "- [3. Knowledge Graph Construction & Visualization](#3-knowledge-graph-construction--visualization)\n",
    "- [4. LLM-based Relation Extraction](#4-llm-based-relation-extraction)\n",
    "- [5. Application of Knowledge Graph](#5-application-of-knowledge-graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e160262b",
   "metadata": {},
   "source": [
    "## 0. Notebook Settings\n",
    "This section defines display options and utility flags for consistent runs across environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2d6245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Notebook Settings ===\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration for project and data directories\n",
    "DATA_DIR = Path(\"datasets/KMIMIC\")   # <-- UPDATE THIS PATH IF NEEDED\n",
    "\n",
    "print(f\"DATA_DIR:    {DATA_DIR.resolve()}\")\n",
    "\n",
    "# Display options (optional)\n",
    "try:\n",
    "    import pandas as pd\n",
    "    pd.set_option(\"display.max_colwidth\", 120)\n",
    "    pd.set_option(\"display.width\", 120)\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaed0fda",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fb57d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Setup & Imports ===\n",
    "# Installation\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "!pip install --quiet networkx pyvis transformers bitsandbytes accelerate sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab057d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Setup & Imports ===\n",
    "# Import Libraries\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "import re\n",
    "import json\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ab69e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Models and Tokenizer\n",
    "\n",
    "# This cell configures and loads the Large Language Model (LLM) and a Named Entity Recognition (NER) model.\n",
    "# We use the LLM for extracting relationships and the NER model to identify medical entities.\n",
    "\n",
    "# --- LLM Configuration ---\n",
    "# Set up cache directories for Hugging Face models\n",
    "os.environ['HF_DATASETS_CACHE'] = './transformers_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = './transformers_cache'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Hugging Face access token (replace with your own if necessary)\n",
    "access_token = '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'\n",
    "model_id = \"epfl-llm/meditron-7b\" \n",
    "\n",
    "# Configuration for loading the model in 4-bit precision to save memory\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    token=access_token\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f912c49",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6aa64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Data Loading ===\n",
    "# --- Configuration: File Paths ---\n",
    "# Update these paths if your files are in a different directory.\n",
    "# We will focus on a few key tables to demonstrate the concept.\n",
    "DATA_DIR = 'datasets/KMIMIC/' # <-- IMPORTANT: UPDATE THIS PATH IF NEEDED\n",
    "file_paths = {\n",
    "    \"patients\": f\"{DATA_DIR}patients.csv\",\n",
    "    \"admissions\": f\"{DATA_DIR}admissions.csv\",\n",
    "    \"diagnoses\": f\"{DATA_DIR}diagnoses_icd.csv\",\n",
    "    \"procedures\": f\"{DATA_DIR}procedures_icd.csv\",\n",
    "    \"prescriptions\": f\"{DATA_DIR}prescriptions.csv\",\n",
    "    #\"labs\": f\"{DATA_DIR}labevents.csv\"\n",
    "    #\"d_diagnoses\": f\"{DATA_DIR}diagnoses_icd.csv\",\n",
    "    #\"d_procedures\": f\"{DATA_DIR}procedures_icd.csv\"\n",
    "}\n",
    "dataframes = {}\n",
    "\n",
    "# --- Data Loading ---\n",
    "try:\n",
    "    print(\"Loading KMIMIC tabular data...\")\n",
    "    for name, path in file_paths.items():\n",
    "        print(f\" - Loading {path}...\")\n",
    "        dataframes[name] = pd.read_csv(path, on_bad_lines='skip')\n",
    "    print(\"\\nAll data loaded successfully.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n--- ERROR: File Not Found ---\")\n",
    "    print(f\"Could not find file: {e.filename}\")\n",
    "    print(\"Please ensure all required MIMIC-IV CSV files are in the specified directory.\")\n",
    "    print(\"Required files: patients.csv.gz, admissions.csv.gz, diagnoses_icd.csv.gz, procedures_icd.csv.gz, prescriptions.csv.gz, d_icd_diagnoses.csv.gz, d_icd_procedures.csv.gz\")\n",
    "    dataframes = {} # Clear dataframes to prevent further errors\n",
    "\n",
    "\n",
    "# --- Match ICD9CM_PROC code to description for procedures table\n",
    "# ICD9CM-PROC table to match code and description\n",
    "# Download from: https://www.cms.gov/medicare/coding-billing/icd-10-codes/icd-9-cm-diagnosis-procedure-codes-abbreviated-and-full-code-titles\n",
    "# Then save as 'icd9cm_proc.txt' in the same directory as this notebook.\n",
    "# Read txt file line by line and parse\n",
    "with open('icd9cm_proc.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Extract code (first 4 chars) and description (from 5th char onwards, stripping spaces)\n",
    "data = []\n",
    "for line in lines:\n",
    "    code = line[:4]\n",
    "    description = line[4:].strip()\n",
    "    data.append([code, description])\n",
    "\n",
    "# Create DataFrame\n",
    "icd9proc = pd.DataFrame(data, columns=['code', 'description'])\n",
    "\n",
    "# Convert ICD-9-PROC code to 4-digit format (ex. '3.20' -> '0320', '3.2' -> '0320')\n",
    "def convert_code(code):\n",
    "    code = \"{:.2f}\".format(code)\n",
    "    if '.' in code:\n",
    "        parts = code.split('.')\n",
    "        first, second = parts\n",
    "        # Pad first part with leading zero if length is 1\n",
    "        if len(first) == 1:\n",
    "            first = '0' + first\n",
    "        # Ensure second part is two digits (if needed)\n",
    "        second = second.zfill(2)\n",
    "        return first + second\n",
    "    else:\n",
    "        # If no dot, pad to 4 digits as general rule\n",
    "        return code.zfill(4)\n",
    "\n",
    "\n",
    "dataframes[\"procedures\"][\"icd_code_4\"] = dataframes[\"procedures\"]['icd_code'].apply(convert_code)\n",
    "dataframes[\"procedures\"][\"opname\"] = dataframes[\"procedures\"].merge(icd9proc, left_on='icd_code_4', right_on='code', how='left')['description']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9003550",
   "metadata": {},
   "source": [
    "## 3. Knowledge Graph Construction & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Knowledge Graph Construction ===\n",
    "# To make the graph manageable for this demonstration, we will select a small sample of patients.\n",
    "\n",
    "if dataframes:\n",
    "    # --- Sample the Data ---\n",
    "    num_patients_to_process = 5 # Increase for a larger, more detailed graph\n",
    "    sample_patient_ids = dataframes[\"patients\"].head(num_patients_to_process)['subject_id']\n",
    "    \n",
    "    # Filter all relevant tables for the sampled patients\n",
    "    for name in [\"patients\", \"admissions\", \"prescriptions\"]:\n",
    "        dataframes[name] = dataframes[name][dataframes[name]['subject_id'].isin(sample_patient_ids)]\n",
    "\n",
    "    sample_admission_ids = dataframes[\"admissions\"]['hadm_id']\n",
    "    # Filter tables that link to admissions\n",
    "    for name in [\"diagnoses\", \"procedures\"]:#, \"labs\"]:\n",
    "        dataframes[name] = dataframes[name][dataframes[name]['hadm_id'].isin(sample_admission_ids)]\n",
    "\n",
    "    print(f\"Sampled data for {len(sample_patient_ids)} patients.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388d1aa5",
   "metadata": {},
   "source": [
    "### Knowledge Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282fbe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Knowledge Graph Construction & Visualization ===\n",
    "# This cell constructs the knowledge graph by defining nodes (patients, admissions, diagnoses, etc.)\n",
    "# and edges based on the relationships (foreign keys) in the tables. We use a MultiDiGraph\n",
    "# to allow for multiple instances of the same event (e.g., multiple lab tests of the same type).\n",
    "\n",
    "if dataframes:\n",
    "    G = nx.MultiDiGraph() # Initialize a directed graph that can have parallel edges\n",
    "\n",
    "    # --- Add Patient Nodes ---\n",
    "    for _, row in dataframes[\"patients\"].iterrows():\n",
    "        patient_id = f\"p_{row['subject_id']}\"\n",
    "        G.add_node(patient_id, label=f\"Patient {row['subject_id']}\", type='patient', gender=row['sex'], anchor_age=row['anchor_age'])\n",
    "\n",
    "    # --- Add Admission Nodes and Patient-Admission Edges ---\n",
    "    for _, row in dataframes[\"admissions\"].iterrows():\n",
    "        patient_id = f\"p_{row['subject_id']}\"\n",
    "        admission_id = f\"a_{row['hadm_id']}\"\n",
    "        G.add_node(admission_id, label=f\"Admission {row['hadm_id']}\", type='admission', admission_type=row['admission_type'], ethnicity=row['ethnicity'], insurance=row['insurance'])\n",
    "        G.add_edge(patient_id, admission_id, label='HAD_ADMISSION')\n",
    "\n",
    "    # --- Add Diagnosis Nodes and Admission-Diagnosis Edges ---\n",
    "    for _, row in dataframes[\"diagnoses\"].dropna(subset=['icd_name']).iterrows():\n",
    "        admission_id = f\"a_{row['hadm_id']}\"\n",
    "        diag_id = f\"d_{row['icd_code']}_{row['icd_version']}\" # Unique ID for diagnosis code\n",
    "        G.add_node(diag_id, label=row['icd_name'], type='diagnosis', icd_version=row['icd_version'])\n",
    "        G.add_edge(admission_id, diag_id, label='DIAGNOSED_WITH')\n",
    "\n",
    "    # --- Add Procedure Nodes and Admission-Procedure Edges ---\n",
    "    for _, row in dataframes[\"procedures\"].dropna(subset=['icd_code']).iterrows():\n",
    "        admission_id = f\"a_{row['hadm_id']}\"\n",
    "        proc_id = f\"pr_{row['icd_code']}_{row['icd_version']}\" # Unique ID for procedure code\n",
    "        G.add_node(proc_id, label=f\"{row['opname']}\", type='procedure', icd_version=row['icd_version'])\n",
    "        G.add_edge(admission_id, proc_id, label='HAD_PROCEDURE')\n",
    "        \n",
    "    # --- Add Prescription Nodes and Patient-Prescription Edges ---\n",
    "    for _, row in dataframes[\"prescriptions\"].dropna(subset=[\"drug(English)\"]).head(30).iterrows(): # Sample to keep graph clean\n",
    "        patient_id = f\"p_{row['subject_id']}\"\n",
    "        drug_name = str(row['drug(English)']).lower()\n",
    "        if not G.has_node(drug_name):\n",
    "            G.add_node(drug_name, label=drug_name.title(), type='prescription')\n",
    "        G.add_edge(patient_id, drug_name, label='PRESCRIBED')\n",
    "\n",
    "    # --- Add Lab Event Nodes and Admission-Lab Edges ---\n",
    "    if False:\n",
    "        for _, row in dataframes[\"labs\"].dropna(subset=['itemid', 'value']).head(50).iterrows(): # Sample to keep graph clean\n",
    "            admission_id = f\"a_{row['hadm_id']}\"\n",
    "            if G.has_node(admission_id):\n",
    "                lab_item_id = f\"l_{row['itemid']}\"\n",
    "                # For this demo, we label lab items by their ID. A real-world application\n",
    "                # would merge this with the d_labitems.csv table for friendly names.\n",
    "                if not G.has_node(lab_item_id):\n",
    "                    G.add_node(lab_item_id, label=f\"Lab Item {row['itemid']}\", type='lab_item')\n",
    "                # The edge represents the actual test event and its result\n",
    "                G.add_edge(admission_id, lab_item_id, label='HAD_LAB_TEST', value=row['value'], unit=row['valueuom'], flag=row.get('flag'))\n",
    "\n",
    "    print(f\"\\nKnowledge Graph built successfully.\")\n",
    "    print(f\" - Nodes: {G.number_of_nodes()}\")\n",
    "    print(f\" - Edges: {G.number_of_edges()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607f50d0",
   "metadata": {},
   "source": [
    "### Visualize the Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3031b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Graph Construction & Visualization ===\n",
    "# This cell creates an interactive HTML visualization of the graph.\n",
    "# Nodes are colored by type for clarity.\n",
    "\n",
    "if 'G' in locals() and G.number_of_nodes() > 0:\n",
    "    net = Network(notebook=True, height='800px', width='100%', bgcolor='#222222', font_color='white', cdn_resources='in_line', directed=True)\n",
    "\n",
    "    # Define colors for different node types\n",
    "    color_map = {\n",
    "        'patient': '#007bff',      # Blue\n",
    "        'admission': '#28a745',   # Green\n",
    "        'diagnosis': '#dc3545',   # Red\n",
    "        'procedure': '#ffc107',   # Yellow\n",
    "        'prescription': '#6f42c1',# Purple\n",
    "        'lab_item': '#17a2b8'     # Teal\n",
    "    }\n",
    "\n",
    "    # Add nodes to the Pyvis network with appropriate styling\n",
    "    for node, data in G.nodes(data=True):\n",
    "        node_type = data.get('type', 'N/A')\n",
    "        net.add_node(node, label=data.get('label', node), color=color_map.get(node_type, '#adb5bd'), title=f\"Type: {node_type}\")\n",
    "\n",
    "    # Add edges\n",
    "    for source, target, data in G.edges(data=True):\n",
    "        net.add_edge(source, target, label=data.get('label', ''))\n",
    "\n",
    "    # Set physics options for a better layout\n",
    "    net.set_options(\"\"\"\n",
    "    var options = {\n",
    "      \"nodes\": { \"font\": { \"size\": 14 } },\n",
    "      \"edges\": { \"font\": { \"align\": \"top\", \"size\": 12 }, \"smooth\": { \"type\": \"dynamic\" } },\n",
    "      \"physics\": {\n",
    "        \"forceAtlas2Based\": { \"gravitationalConstant\": -50, \"centralGravity\": 0.01, \"springLength\": 150 },\n",
    "        \"minVelocity\": 0.75, \"solver\": \"forceAtlas2Based\"\n",
    "      }\n",
    "    }\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"Generating interactive knowledge graph visualization...\")\n",
    "    display(net.show(\"tabular_knowledge_graph_revised.html\"))\n",
    "    print(\"Interactive graph 'tabular_knowledge_graph_revised.html' has been saved to your directory.\")\n",
    "else:\n",
    "    print(\"No graph was built, so visualization is skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07610a44",
   "metadata": {},
   "source": [
    "## 4. LLM-based Relation Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6a3fac",
   "metadata": {},
   "source": [
    "### Using LLM to Extract more relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5652c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Data Loading ===\n",
    "def extract_relationships(response: str):\n",
    "    \"\"\"\n",
    "    Extracts a balanced JSON object from an LLM response (even if trimmed),\n",
    "    cleans it (removes trailing commas), and returns the `relationships` list.\n",
    "    \"\"\"\n",
    "    # 1) Locate the first opening brace\n",
    "    start = response.find('{')\n",
    "    if start == -1:\n",
    "        return []\n",
    "\n",
    "    # 2) Extract substring from that point\n",
    "    substr = response[start:]\n",
    "\n",
    "    # 3) Find the matching closing brace by counting\n",
    "    depth = 0\n",
    "    end_idx = None\n",
    "    for i, ch in enumerate(substr):\n",
    "        if ch == '{':\n",
    "            depth += 1\n",
    "        elif ch == '}':\n",
    "            depth -= 1\n",
    "            if depth == 0:\n",
    "                end_idx = i + 1\n",
    "                break\n",
    "\n",
    "    # If we never closed, take everything (best-effort)\n",
    "    raw_json = substr[:end_idx] if end_idx is not None else substr\n",
    "\n",
    "    # 4) Clean up whitespace and trailing commas\n",
    "    text = raw_json.strip()\n",
    "    text = re.sub(r\",\\s*([}\\]])\", r\"\\1\", text)\n",
    "\n",
    "    # 5) Try parsing\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON parse error: {e}\")\n",
    "        print(\"Attempted JSON:\\n\", text)\n",
    "        return []\n",
    "\n",
    "    # 6) Return relationships list\n",
    "    rels = data.get(\"relationships\", [])\n",
    "    return rels if isinstance(rels, list) else []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d12a72c",
   "metadata": {},
   "source": [
    "### Build the Knowledge Graph from Tabular and LLM-Extracted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fbebac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Data Loading ===\n",
    "# This cell constructs the knowledge graph in two stages:\n",
    "# 1. Structural Layer: Build a base graph from the explicit relationships (foreign keys) in the tables.\n",
    "# 2. Semantic Layer: Use the LLM to infer and add more complex relationships between entities within the context of a single admission.\n",
    "\n",
    "def get_llm_response(prompt):\n",
    "    \"\"\"\n",
    "    Helper function to get a response from the loaded LLM.\n",
    "    This version correctly decodes only the newly generated tokens.\n",
    "    \"\"\"\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids_length = inputs.input_ids.shape[1] # len(inputs[\"input_ids\"])\n",
    "\n",
    "    # Generate a response\n",
    "    outputs = model.generate(**inputs, max_new_tokens=1024, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # Decode only the tokens that were generated after the prompt\n",
    "    response_ids = outputs[0][input_ids_length:]\n",
    "    return tokenizer.decode(response_ids, skip_special_tokens=True)\n",
    "\n",
    "if dataframes:\n",
    "    # Use a standard DiGraph to prevent duplicate edges between nodes\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # --- STAGE 1: Build Structural Layer ---\n",
    "    # (Same as before, building the skeleton of the graph)\n",
    "    for _, row in dataframes[\"patients\"].iterrows():\n",
    "        G.add_node(f\"p_{row['subject_id']}\", label=f\"Patient {row['subject_id']}\", type='patient', gender=row['sex'], anchor_age=row['anchor_age'])\n",
    "    for _, row in dataframes[\"admissions\"].iterrows():\n",
    "        G.add_node(f\"a_{row['hadm_id']}\", label=f\"Admission {row['hadm_id']}\", type='admission', admission_type=row['admission_type'], ethnicity=row['ethnicity'])\n",
    "        G.add_edge(f\"p_{row['subject_id']}\", f\"a_{row['hadm_id']}\", label='HAD_ADMISSION')\n",
    "    for _, row in dataframes[\"diagnoses\"].dropna(subset=['icd_name']).iterrows():\n",
    "        diag_id = f\"d_{row['icd_code']}_{row['icd_version']}\"\n",
    "        G.add_node(diag_id, label=row['icd_name'], type='diagnosis')\n",
    "        G.add_edge(f\"a_{row['hadm_id']}\", diag_id, label='DIAGNOSED_WITH')\n",
    "    for _, row in dataframes[\"procedures\"].dropna(subset=['icd_code']).iterrows():\n",
    "        proc_id = f\"pr_{row['icd_code']}_{row['icd_version']}\"\n",
    "        G.add_node(proc_id, label=f\"Proc: {row['icd_code']}\", type='procedure')\n",
    "        G.add_edge(f\"a_{row['hadm_id']}\", proc_id, label='HAD_PROCEDURE')\n",
    "    for _, row in dataframes[\"prescriptions\"].dropna(subset=[\"drug(English)\"]).iterrows():\n",
    "        drug_name = str(row['drug(English)']).lower()\n",
    "        # Use drug name as node ID for simplicity, ensuring one node per drug\n",
    "        drug_node_id = drug_name\n",
    "        if not G.has_node(drug_node_id):\n",
    "            G.add_node(drug_node_id, label=drug_name.title(), type='prescription')\n",
    "        # This edge connects patient to prescription, not admission\n",
    "        G.add_edge(f\"p_{row['subject_id']}\", drug_node_id, label='PRESCRIBED')\n",
    "\n",
    "    print(\"Stage 1 (Structural Graph) Complete.\")\n",
    "    print(f\" - Nodes: {G.number_of_nodes()}, Edges: {G.number_of_edges()}\")\n",
    "\n",
    "    # --- STAGE 2: Add Semantic Layer with LLM ---\n",
    "    print(\"\\nStarting Stage 2: Adding semantic relationships with LLM...\")\n",
    "    for hadm_id in dataframes[\"admissions\"]['hadm_id'][:1]:\n",
    "        # Gather all entities for this admission (Sample 3 cases for the demonstration)\n",
    "        diagnoses = dataframes[\"diagnoses\"][dataframes[\"diagnoses\"]['hadm_id'] == hadm_id]['icd_name'].unique().tolist()[:3]\n",
    "        procedures = [f\"Proc: {c}\" for c in dataframes[\"procedures\"][dataframes[\"procedures\"]['hadm_id'] == hadm_id]['icd_code'].unique().tolist()][:3]\n",
    "        drugs = dataframes[\"prescriptions\"][dataframes[\"prescriptions\"]['hadm_id'] == hadm_id]['drug(English)'].dropna().str.title().unique().tolist()[:3]\n",
    "        \n",
    "        if not diagnoses or (not procedures and not drugs):\n",
    "            continue\n",
    "\n",
    "        # This revised prompt strictly enforces a JSON output and provides structured lists of entities.\n",
    "        prompt = f\"\"\"\n",
    "        <s>[INST]\n",
    "        <<SYS>>\n",
    "        You are a clinical knowledge extraction expert. Your task is to identify plausible relationships between the provided lists of diagnoses, procedures, and drugs from a single patient admission.\n",
    "\n",
    "        **Instructions:**\n",
    "        1.  Analyze the lists below to find connections. For example, a drug might treat a diagnosis, or a procedure might be indicated by a diagnosis.\n",
    "        2.  Create relationship triplets in the format `[ENTITY 1, RELATION, ENTITY 2]`.\n",
    "        3.  Valid relations include `TREATS`, `MANAGES`, `IS_INDICATION_FOR`, `SIDE_EFFECT`, `DIAGNOSES`.\n",
    "        4.  The output MUST be a valid JSON object with a single key \"relationships\", which contains a list of the identified triplets.\n",
    "        5.  If no plausible relationships can be inferred, return an empty list: `{{\"relationships\": []}}`.\n",
    "        6.  There should be maximum one relation between two nodes.\n",
    "        <</SYS>>\n",
    "\n",
    "        **Entities from Patient Admission:**\n",
    "\n",
    "        * **Diagnoses:** {json.dumps(diagnoses)}\n",
    "        * **Procedures:** {json.dumps(procedures)}\n",
    "        * **Drugs:** {json.dumps(drugs)}\n",
    "\n",
    "        You should not exceed more than 10 triplets of relationships. Answer the output in JSON format.\n",
    "\n",
    "        **JSON Output:**\n",
    "        [/INST]\n",
    "        \"\"\"\n",
    "        \n",
    "        response = get_llm_response(prompt)\n",
    "        \n",
    "        # Extract triplets from the LLM response with improved, more robust parsing\n",
    "        try:\n",
    "            data = extract_relationships(response)\n",
    "            # Find the JSON object within the response string\n",
    "   \n",
    "            #triplets = data.get(\"relationships\", []) # Safely get the list of triplets\n",
    "            \n",
    "            if isinstance(triplets, list):\n",
    "                for triplet in triplets:\n",
    "                    if isinstance(triplet, list) and len(triplet) == 3:\n",
    "                        # Find the corresponding nodes in the graph using case-insensitive matching\n",
    "                        source_label = triplet[0]\n",
    "                        target_label = triplet[2]\n",
    "                        \n",
    "                        source_node = [n for n, d in G.nodes(data=True) if d.get('label','').lower() == source_label.lower()]\n",
    "                        target_node = [n for n, d in G.nodes(data=True) if d.get('label','').lower() == target_label.lower()]\n",
    "                        \n",
    "                        if source_node and target_node:\n",
    "                            # Add the new semantic edge\n",
    "                            G.add_edge(source_node[0], target_node[0], label=triplet[1].upper(), type='semantic')\n",
    "                            print(f\"  + Added semantic edge: {source_label} --[{triplet[1].upper()}]--> {target_label}\")\n",
    "        except json.JSONDecodeError:\n",
    "            # This handles the specific error where the LLM response is not valid JSON.\n",
    "            print(f\"  - Could not parse LLM response. The model did not return a valid JSON object.\")\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            # Catch any other unexpected errors during the process\n",
    "            print(f\"  - An unexpected error occurred during semantic edge extraction: {e}\")\n",
    "            pass\n",
    "\n",
    "    print(\"\\nKnowledge Graph build complete (Structural + Semantic).\")\n",
    "    print(f\" - Final Nodes: {G.number_of_nodes()}, Final Edges: {G.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0c789b",
   "metadata": {},
   "source": [
    "### Visualize the Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed56a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Graph Construction & Visualization ===\n",
    "# This cell creates an interactive HTML visualization of the graph.\n",
    "# Nodes are colored by type for clarity.\n",
    "\n",
    "if 'G' in locals() and G.number_of_nodes() > 0:\n",
    "    net = Network(notebook=True, height='800px', width='100%', bgcolor='#222222', font_color='white', cdn_resources='in_line', directed=True)\n",
    "\n",
    "    # Define colors for different node types\n",
    "    color_map = {\n",
    "        'patient': '#007bff',      # Blue\n",
    "        'admission': '#28a745',   # Green\n",
    "        'diagnosis': '#dc3545',   # Red\n",
    "        'procedure': '#ffc107',   # Yellow\n",
    "        'prescription': '#6f42c1',# Purple\n",
    "        'lab_item': '#17a2b8'     # Teal\n",
    "    }\n",
    "\n",
    "    # Add nodes to the Pyvis network with appropriate styling\n",
    "    for node, data in G.nodes(data=True):\n",
    "        node_type = data.get('type', 'N/A')\n",
    "        net.add_node(node, label=data.get('label', node), color=color_map.get(node_type, '#adb5bd'), title=f\"Type: {node_type}\")\n",
    "\n",
    "    # Add edges\n",
    "    for source, target, data in G.edges(data=True):\n",
    "        net.add_edge(source, target, label=data.get('label', ''))\n",
    "\n",
    "    # Set physics options for a better layout\n",
    "    net.set_options(\"\"\"\n",
    "    var options = {\n",
    "      \"nodes\": { \"font\": { \"size\": 14 } },\n",
    "      \"edges\": { \"font\": { \"align\": \"top\", \"size\": 12 }, \"smooth\": { \"type\": \"dynamic\" } },\n",
    "      \"physics\": {\n",
    "        \"forceAtlas2Based\": { \"gravitationalConstant\": -50, \"centralGravity\": 0.01, \"springLength\": 150 },\n",
    "        \"minVelocity\": 0.75, \"solver\": \"forceAtlas2Based\"\n",
    "      }\n",
    "    }\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"Generating interactive knowledge graph visualization...\")\n",
    "    display(net.show(\"tabular_knowledge_graph_revised.html\"))\n",
    "    print(\"Interactive graph 'tabular_knowledge_graph_revised.html' has been saved to your directory.\")\n",
    "else:\n",
    "    print(\"No graph was built, so visualization is skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1871f816",
   "metadata": {},
   "source": [
    "## 5. Application of Knowledge Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ba6594",
   "metadata": {},
   "source": [
    "### Advanced GraphRAG Query Demonstrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82931c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Data Loading ===\n",
    "# Cell 9: Application - GraphRAG for Clinical Questions\n",
    "#\n",
    "# This cell demonstrates a GraphRAG (Retrieval-Augmented Generation) system.\n",
    "# 1. A user asks a question in natural language.\n",
    "# 2. The LLM translates the question into a structured graph query.\n",
    "# 3. A Python function executes this query against our NetworkX graph.\n",
    "# 4. The LLM receives the query results and synthesizes a final, human-readable answer.\n",
    "\n",
    "def execute_graph_query(graph, query_plan):\n",
    "    \"\"\"Executes a structured query plan against the NetworkX graph.\"\"\"\n",
    "    nodes = list(graph.nodes(data=True))\n",
    "    results = []\n",
    "    \n",
    "    # Find nodes matching the criteria\n",
    "    for node_id, properties in nodes:\n",
    "        if properties.get('type') == query_plan['target_node']['type'] and query_plan['target_node']['property_value'].lower() in properties.get('label', '').lower():\n",
    "            # Now find the connected nodes based on the query plan\n",
    "            for neighbor in graph.predecessors(node_id):\n",
    "                edge_data = graph.get_edge_data(neighbor, node_id)\n",
    "                if graph.nodes[neighbor].get('type') == query_plan['connected_node']['type'] and edge_data.get('label') == query_plan['edge']['label']:\n",
    "                    results.append(graph.nodes[neighbor].get('label'))\n",
    "    return results\n",
    "\n",
    "def ask_question_with_graphrag(graph, question):\n",
    "    \"\"\"Orchestrates the GraphRAG process.\"\"\"\n",
    "    print(f\"User Question: \\\"{question}\\\"\")\n",
    "    \n",
    "    # 1. Use LLM to translate NLQ to a structured query plan\n",
    "    plan_instruction = f\"\"\"\n",
    "    Convert the following question into a JSON query plan to be executed on a knowledge graph.\n",
    "    The graph has nodes with types 'patient', 'prescription', 'diagnosis'.\n",
    "    Edges have labels like 'PRESCRIBED', 'DIAGNOSED_WITH'.\n",
    "\n",
    "    Question: \"{question}\"\n",
    "\n",
    "    JSON Query Plan:\n",
    "    \"\"\"\n",
    "    plan_prompt = f\"\"\"<s>[INST]\n",
    "<<SYS>>\n",
    "You are a helpful assistant that converts natural language questions into structured JSON query plans.\n",
    "<</SYS>>\n",
    "\n",
    "{plan_instruction}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "    \n",
    "    response = get_llm_response(plan_prompt)\n",
    "    \n",
    "    try:\n",
    "        query_plan_str = re.search(r'\\{.*\\}', response, re.DOTALL).group(0)\n",
    "        query_plan = json.loads(query_plan_str)\n",
    "        print(f\"\\nStep 1: Generated Query Plan:\\n{json.dumps(query_plan, indent=2)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not parse query plan from LLM response: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Execute the query on the graph\n",
    "    query_results = execute_graph_query(graph, query_plan)\n",
    "    print(f\"\\nStep 2: Retrieved {len(query_results)} results from the graph.\")\n",
    "    if query_results:\n",
    "        print(\"Results:\", query_results)\n",
    "\n",
    "    # 3. Use LLM to synthesize the final answer\n",
    "    answer_instruction = f\"\"\"\n",
    "    Based on the following information retrieved from a clinical knowledge graph, provide a concise answer to the user's original question.\n",
    "\n",
    "    Original Question: \"{question}\"\n",
    "    Retrieved Information: \"{', '.join(query_results) if query_results else 'No information found.'}\"\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    answer_prompt = f\"\"\"<s>[INST]\n",
    "<<SYS>>\n",
    "You are a helpful biomedical assistant that answers questions based on provided data.\n",
    "<</SYS>>\n",
    "\n",
    "{answer_instruction}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "    final_answer = get_llm_response(answer_prompt)\n",
    "    \n",
    "    print(f\"\\nStep 3: Synthesized Final Answer:\\n{final_answer}\")\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d9ed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LLM-based Information Extraction ===\n",
    "# This cell demonstrates how to use the `ask_question_with_graphrag` function\n",
    "# to answer different types of clinical and operational questions.\n",
    "\n",
    "if 'G' in locals() and G.number_of_nodes() > 0:\n",
    "    print(\"--- Advanced GraphRAG Query Demonstrations ---\")\n",
    "    print(\"Running a series of example questions against the knowledge graph...\")\n",
    "\n",
    "    # --- Example 1: Patient Cohort Analysis ---\n",
    "    # This query identifies a specific group of patients based on their clinical characteristics.\n",
    "    # The LLM needs to create a query plan that finds 'Pneumonia' nodes, then finds connected\n",
    "    # 'admission' nodes, and finally checks the 'anchor_age' property on the connected 'patient' nodes.\n",
    "    # Note: The current `execute_graph_query` is simple and may not handle the age filter directly.\n",
    "    # The LLM's power is in its ability to formulate a plan, even if the execution engine is basic.\n",
    "    print(\"\\n\\n\" + \"=\"*50)\n",
    "    print(\"Example 1: Patient Cohort Analysis\")\n",
    "    print(\"=\"*50)\n",
    "    cohort_question = \"Which patients older than 70 were diagnosed with Pneumonia?\"\n",
    "    ask_question_with_graphrag(G, cohort_question)\n",
    "\n",
    "    # --- Example 2: Treatment Pathway and Efficacy Questions ---\n",
    "    # This query leverages the semantic 'TREATS' relationships added by the LLM in Stage 2.\n",
    "    # It seeks to understand common treatment patterns for a specific condition.\n",
    "    print(\"\\n\\n\" + \"=\"*50)\n",
    "    print(\"Example 2: Treatment Pathway Inquiry\")\n",
    "    print(\"=\"*50)\n",
    "    treatment_question = \"How is Atrial Fibrillation treated?\"\n",
    "    # For this to work well, the LLM needs to generate a query plan that looks for 'TREATS'\n",
    "    # edges pointing TO the 'Atrial Fibrillation' diagnosis node.\n",
    "    ask_question_with_graphrag(G, treatment_question)\n",
    "\n",
    "    # --- Example 3: Exploratory and Complex Queries ---\n",
    "    # This is a more complex, multi-hop query that the simple `execute_graph_query` function\n",
    "    # cannot handle in a single pass. However, we can demonstrate how the LLM attempts to\n",
    "    # break it down. An advanced implementation would use an iterative query engine.\n",
    "    print(\"\\n\\n\" + \"=\"*50)\n",
    "    print(\"Example 3: Exploratory / Multi-Hop Query\")\n",
    "    print(\"=\"*50)\n",
    "    exploratory_question = \"What are the common procedures for patients who are prescribed both Insulin and Metformin?\"\n",
    "    # The LLM would need to generate a complex plan:\n",
    "    # 1. Find all patients prescribed Insulin.\n",
    "    # 2. Find all patients prescribed Metformin.\n",
    "    # 3. Find the intersection of these two patient groups.\n",
    "    # 4. For that intersection, find all connected procedures.\n",
    "    ask_question_with_graphrag(G, exploratory_question)\n",
    "\n",
    "else:\n",
    "    print(\"Knowledge graph 'G' not found. Please run the previous cells to build the graph first.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
