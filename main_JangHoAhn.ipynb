{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1669ff4",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bbc33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules from the HuggingFace transformers library for NLP tasks and model handling\n",
    "from transformers import ( \n",
    "    AutoTokenizer,                # For tokenizing input text for transformer models\n",
    "    AutoModelForCausalLM,         # For loading causal language models (e.g., GPT-style)\n",
    "    BitsAndBytesConfig,           # For quantization and memory-efficient model loading\n",
    "    TrainingArguments,            # For specifying training hyperparameters\n",
    "    AutoModel,                    # For loading generic transformer models\n",
    "    Trainer,                      # For training transformer models\n",
    "    pipeline,                     # For easy-to-use inference pipelines\n",
    "    DataCollatorForSeq2Seq,       # For data collation in sequence-to-sequence tasks\n",
    "    AutoModelForSeq2SeqLM         # For loading sequence-to-sequence language models (e.g., T5, BART)\n",
    ")\n",
    "\n",
    "import polars as pl                # High-performance DataFrame library, alternative to pandas\n",
    "import warnings                    # For controlling warning messages\n",
    "from tqdm import tqdm              # For progress bars in loops\n",
    "import glob, os                    # For file and directory operations\n",
    "from PyPDF2 import PdfReader       # For reading PDF files\n",
    "from sklearn.model_selection import train_test_split  # For splitting datasets\n",
    "import pandas as pd                # Data analysis and manipulation tool\n",
    "import numpy as np                 # Numerical computing library\n",
    "import torch                       # PyTorch for deep learning\n",
    "import torch._dynamo               # For PyTorch optimization and compilation\n",
    "from datasets import Dataset       # HuggingFace datasets library for handling datasets\n",
    "import json                        # For JSON file operations\n",
    "import faiss                       # For efficient similarity search and clustering of dense vectors\n",
    "import re                          # For regular expressions\n",
    "from peft import (                 # Parameter-Efficient Fine-Tuning (PEFT) utilities\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    "    TaskType, \n",
    "    PeftModel, \n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction  # For BLEU score calculation (NLP evaluation)\n",
    "from rouge_score import rouge_scorer                                 # For ROUGE score calculation (NLP evaluation)\n",
    "from pathlib import Path                                             # For object-oriented filesystem paths\n",
    "from sentence_transformers import CrossEncoder                       # For cross-encoder models (sentence similarity)\n",
    "from sklearn.metrics.pairwise import cosine_similarity as skl_cosine # For cosine similarity between vectors\n",
    "from typing import List, Tuple                                       # For type hinting\n",
    "\n",
    "# Function to split text into overlapping chunks for processing (e.g., for LLM context windows)\n",
    "def chunk_text(text, max_length=500, overlap=50):\n",
    "    sentences = text.split(\". \")  # Naive sentence splitting by period\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for sent in sentences:\n",
    "        # If adding the next sentence does not exceed max_length, append it\n",
    "        if len(current_chunk) + len(sent) + 2 <= max_length:\n",
    "            current_chunk += sent + \". \"\n",
    "        else:\n",
    "            # Otherwise, save the current chunk and start a new one with overlap\n",
    "            chunks.append(current_chunk.strip())\n",
    "            overlap_words = current_chunk.split()[-overlap:]  # Get last 'overlap' words\n",
    "            current_chunk = \" \".join(overlap_words) + \" \" + sent + \". \"\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "# Function to compute cosine similarity between two vectors (e.g., sentence embeddings)\n",
    "def cosine_similarity(a, b):\n",
    "    a_norm = a / (np.linalg.norm(a) + 1e-8)  # Normalize vector a\n",
    "    b_norm = b / (np.linalg.norm(b) + 1e-8)  # Normalize vector b\n",
    "    return float(np.dot(a_norm, b_norm))      # Return cosine similarity\n",
    "\n",
    "# Suppress all warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Suppress PyTorch Dynamo errors (for experimental/unstable features)\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "# Model name for a lightweight LLM suitable for medical tasks and Tesla T4 GPU environment\n",
    "MODEL_NAME = \"Intelligent-Internet/II-Medical-8B-1706\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb33663",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac56c6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# (Optional) Example code for filtering document types related to infection\n",
    "doc_type = pl.read_excel('datasets_folder/SNUHNOTE/1.0.1/document_type_level_mst.xlsx')\n",
    "doc_type = doc_type.filter(\n",
    "    (pl.col(\"doc_type_id\") == \"D008\") & \n",
    "    (pl.col(\"mdfm_name\").str.contains(\"감염\"))\n",
    ")\n",
    "# mdfm_id = 41813, 41814, 42345 refers to 타과의뢰회신 (consultation requests & replies)\n",
    "'''\n",
    "\n",
    "dfs = []\n",
    "\n",
    "# Loop over 7 Excel files containing consultation data\n",
    "for i in tqdm(range(1, 8)):\n",
    "    # Read each file into a Polars DataFrame\n",
    "    df = pl.read_excel(f'datasets_folder/SNUHNOTE/1.0.1/4_DGNS/DGNS_{i}.xlsx')\n",
    "\n",
    "    # --- Select only 타과의뢰회신 (consultation replies, not requests) passages ---\n",
    "    # Filter rows where level_path indicates 타과의뢰회신, but exclude short approval messages and irrelevant content\n",
    "    df1 = df.filter(\n",
    "        ((pl.col(\"level_path\").str.contains(\"41813\"))|(pl.col(\"level_path\").str.contains(\"41814\"))|(pl.col(\"level_path\").str.contains(\"42345\"))) &\n",
    "        ~(pl.col(\"content\")=='승인하였습니다.\\n') & ~(pl.col(\"content\")=='승인하였습니다. \\n') &  ~(pl.col(\"content\")=='승인하였습니다. \\n\\n') & ~(pl.col(\"content\")=='승인하였습니다.\\n\\n') &\n",
    "        ~(pl.col(\"content\")=='승인하였습니다.\\n\\n\\n') & ~(pl.col(\"content\")=='승인하였습니다. \\n\\n\\n') &\n",
    "        ~(pl.col(\"content\")=='승인하겠습니다.\\n') & ~(pl.col(\"content\")=='승인하겠습니다. \\n') &  ~(pl.col(\"content\")=='승인하겠습니다. \\n\\n') & ~(pl.col(\"content\")=='승인하겠습니다.\\n\\n') &\n",
    "        ~(pl.col(\"content\")=='승인하겠습니다.\\n\\n\\n') & ~(pl.col(\"content\")=='승인하겠습니다. \\n\\n\\n') &\n",
    "        ~(pl.col(\"content\")=='투약 승인하였습니다.\\n') & ~(pl.col(\"content\")=='투약 승인하였습니다. \\n') & ~(pl.col(\"content\")=='투약 승인하였습니다. \\n\\n') &~(pl.col(\"content\")=='투약 승인하였습니다.\\n\\n') &\n",
    "        ~(pl.col(\"content\").str.contains(\"문의\")) & ~(pl.col(\"content\").str.contains(\"의뢰\")) &~(pl.col(\"content\").str.contains(\"취소된 처방\")) &\n",
    "        ((pl.col(\"content\").str.contains(\"추천\"))|(pl.col(\"content\").str.contains(\"권장\"))|(pl.col(\"content\").str.contains(\"승인\"))|(pl.col(\"content\").str.contains(\"고려\"))|\n",
    "        (pl.col(\"content\").str.contains(\"니다\")|(pl.col(\"content\").str.contains(\"권고\"))))\n",
    "    )\n",
    "    # Sort by patient ID and record date\n",
    "    df1 = df1.sort([\"nid\", \"rec_dt_offset\"])\n",
    "    # Group by patient ID, aggregate first record date, first level_path, and concatenate all content for that patient\n",
    "    df1 = df1.group_by(\"nid\").agg([\n",
    "        pl.col(\"rec_dt_offset\").first().alias(\"rec_dt_offset\"),\n",
    "        pl.col(\"level_path\").first().alias(\"level_path\"),\n",
    "        pl.col(\"content\").implode().alias(\"content_list\")\n",
    "    ])\n",
    "    # Join all content into a single string per patient\n",
    "    df1 = df1.with_columns(\n",
    "        pl.col(\"content_list\").list.join(\" \").alias(\"content\")\n",
    "    ).select([\"nid\", \"rec_dt_offset\", \"content\"])\n",
    "\n",
    "    # --- Select only 타과의뢰 (consultation requests, not replies) passages ---\n",
    "    # Filter rows for replies containing certain keywords, but exclude those with recommendation/approval phrases\n",
    "    df2 = df.filter(\n",
    "        ((pl.col(\"level_path\").str.contains(\"41813\"))|(pl.col(\"level_path\").str.contains(\"41814\"))|(pl.col(\"level_path\").str.contains(\"42345\"))) &\n",
    "        ((pl.col(\"content\").str.contains(\"문의\"))|(pl.col(\"content\").str.contains(\"상의\"))|(pl.col(\"content\").str.contains(\"의뢰\"))) &\n",
    "        ~((pl.col(\"content\").str.contains(\"추천\"))|(pl.col(\"content\").str.contains(\"권장\"))|(pl.col(\"content\").str.contains(\"승인\"))|(pl.col(\"content\").str.contains(\"고려\"))|\n",
    "        (pl.col(\"content\").str.contains(\"바랍니다\"))|(pl.col(\"content\").str.contains(\"의뢰주셔서\"))|(pl.col(\"content\").str.contains(\"의뢰 감사\"))|(pl.col(\"content\").str.contains(\"권고\"))|\n",
    "        (pl.col(\"content\").str.contains(\"분 이내\"))|(pl.col(\"content\").str.contains(\"해당없음\"))|(pl.col(\"content\").str.contains(\"니다\")) )\n",
    "    )\n",
    "\n",
    "    # Sort and group by patient ID, aggregate all record dates and content into lists\n",
    "    df2 = df2.sort([\"nid\", \"rec_dt_offset\"])\n",
    "    df2 = df2.group_by(\"nid\").agg([\n",
    "        pl.col(\"rec_dt_offset\").alias(\"rec_dt_offset_list\"),\n",
    "        pl.col(\"content\").alias(\"content_list\")\n",
    "    ])\n",
    "    \n",
    "    # Explode lists so each row is a single (nid, rec_dt_offset, content) pair\n",
    "    df2_long = (\n",
    "        df2\n",
    "        .select([\"nid\", \"rec_dt_offset_list\", \"content_list\"])\n",
    "        .explode([\"rec_dt_offset_list\", \"content_list\"])\n",
    "        .rename({\"rec_dt_offset_list\": \"df2_rec_dt_offset\",\n",
    "                 \"content_list\":        \"cst\"})          \n",
    "        .sort([\"nid\", \"df2_rec_dt_offset\"])\n",
    "    )\n",
    "\n",
    "    # --- Join consultation requests and replies by patient and date ---\n",
    "    # For each request, find the most recent reply (backward join) for the same patient\n",
    "    final_df = (\n",
    "        df1.sort([\"nid\", \"rec_dt_offset\"])               \n",
    "           .join_asof(                                   \n",
    "                df2_long,\n",
    "                left_on=\"rec_dt_offset\",\n",
    "                right_on=\"df2_rec_dt_offset\",\n",
    "                by=\"nid\",\n",
    "                strategy=\"backward\"                      \n",
    "           )\n",
    "           .select([\"nid\", \"rec_dt_offset\", \"content\", \"cst\"])\n",
    "    )\n",
    "    dfs.append(final_df)\n",
    "\n",
    "# Concatenate all dataframes from each file\n",
    "x1 = pl.concat(dfs)\n",
    "# For each patient, keep only the first record (earliest rec_dt_offset)\n",
    "x1 = x1.sort(\"rec_dt_offset\").group_by(\"nid\").first()\n",
    "# Remove rows where there is no reply content\n",
    "x1 = x1.filter(pl.col(\"cst\").is_not_null())\n",
    "\n",
    "# Function to remove the name of the replier at the end of each 타과의뢰회신\n",
    "def remove_im_to_baesang(text):\n",
    "    # Remove text between 'IM' and '배상' (including both), which is a signature pattern\n",
    "    return re.sub(r'IM((?:(?!배상).)*?)배상', '', text, flags=re.DOTALL)\n",
    "\n",
    "# Apply the signature removal function to the 'content' column\n",
    "x1 = x1.with_columns(\n",
    "    pl.col('content').map_elements(remove_im_to_baesang).alias('content')\n",
    ")\n",
    "\n",
    "qa_pairs = []\n",
    "\n",
    "# Generate QA pairs for fine-tuning: question = prompt + attending message, answer = reply content\n",
    "for row in tqdm(x1.iter_rows(named=True), total=x1.height, desc=\"Generating QA pairs\"):\n",
    "    question = (\n",
    "        \"@@@ Task: \\n\"\n",
    "        \"You are an infectious-disease consultant.  Compose ONE brief consultation note as the [@@@ Answer] that obeys **all** rules 1~4 below:\\n\"\n",
    "        \"1. Respond including 3 categories below:\\n\"\n",
    "        \"   • (1) suspected / confirmed pathogen (if any)\\n\"\n",
    "        \"   • (2) recommended medications (if any)\\n\"\n",
    "        \"   • (3) additional labs / cultures (if any)\\n\"\n",
    "        \"2. You must NOT repeat any phrase or idea in each sentence.\\n\"\n",
    "        \"3. Do not copy ANY text from [@@@ Attending physician’s message].\\n\"\n",
    "        \"4. Write in complete sentences and ALWAYS finish each sentence with a period.\\n\\n\"\n",
    "        \"@@@ Attending physician’s message\\n\"\n",
    "        f\"{row['cst']}\\n\\n\"\n",
    "        \"@@@ Answer:\"\n",
    "    )\n",
    "    answer = row[\"content\"]\n",
    "    qa_pairs.append({\n",
    "        \"question\": question,\n",
    "        \"answer\": answer\n",
    "    })\n",
    "\n",
    "# Save the generated QA pairs to a JSON file for later use in LLM fine-tuning\n",
    "with open(\"qa_pairs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(qa_pairs, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b993b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model name for NLLB-200, a multilingual translation model\n",
    "MODEL_NAME = \"facebook/nllb-200-3.3B\"\n",
    "\n",
    "# Load the tokenizer for the model, specifying Korean as the source language\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, src_lang=\"kor_Hang\")\n",
    "\n",
    "# Load the translation model and set it to evaluation mode (no training)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).eval()\n",
    "\n",
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "def translate_ko2en(text: str, max_len: int = 512, num_beams: int = 6) -> str:\n",
    "    \"\"\"\n",
    "    Translate Korean text to English using the NLLB-200 model.\n",
    "    Args:\n",
    "        text (str): Input Korean text.\n",
    "        max_len (int): Maximum length of the output sequence.\n",
    "        num_beams (int): Number of beams for beam search (not used here, but can be added for better translation).\n",
    "    Returns:\n",
    "        str: Translated English text.\n",
    "    \"\"\"\n",
    "    # Tokenize the input text and move tensors to the correct device\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_len,\n",
    "    ).to(device)\n",
    "\n",
    "    # Generate translation using the model, forcing the output language to English\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"eng_Latn\"),  # Set output language to English\n",
    "            max_length=max_len,\n",
    "        )\n",
    "\n",
    "    # Decode the generated tokens to a string and remove special tokens/whitespace\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    \n",
    "# Define source and destination JSON file paths\n",
    "SRC_JSON = Path(\"qa_pairs.json\")\n",
    "DST_JSON = Path(\"qa_pairs_en.json\")\n",
    "\n",
    "# Load the original QA pairs (in Korean) from the source JSON file\n",
    "with SRC_JSON.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Translate the 'answer' field of each QA pair from Korean to English\n",
    "translated = []\n",
    "for item in tqdm(data, desc=\"Translating 'question' and 'answer' fields\"):\n",
    "    new_item = item.copy()  # Keep the original item unchanged\n",
    "    if isinstance(item.get(\"answer\"), str):\n",
    "        # Translate the answer text\n",
    "        translated_answer = translate_ko2en(item[\"answer\"])\n",
    "        # Fix common mistranslation: replace 'investment' with 'administration' if it appears as a word\n",
    "        translated_answer = re.sub(r'(?<=\\S)investment(?=\\S)', 'administration', translated_answer, flags=re.IGNORECASE)\n",
    "        new_item[\"answer\"] = translated_answer\n",
    "    translated.append(new_item)\n",
    "\n",
    "# Save the translated QA pairs to the destination JSON file\n",
    "with DST_JSON.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Saved translated data ➜ {DST_JSON.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7d10db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Settings for LoRA Fine-tuning\n",
    "\n",
    "# Configure 4-bit quantization for memory-efficient model loading\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,            \n",
    "    bnb_4bit_quant_type=\"nf4\",    # Use NormalFloat4 quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Use bfloat16 for computation\n",
    "    bnb_4bit_use_double_quant=True # Double quantization for further compression\n",
    ")\n",
    "\n",
    "# Load the base model with quantization config and your HuggingFace API key\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",            \n",
    "    quantization_config=bnb_cfg,\n",
    "    token = \"YOUR_API_KEY\" # Replace with your own HuggingFace API key\n",
    ")\n",
    "\n",
    "# Prepare the model for k-bit (quantized) training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16,  # Rank of the LoRA update matrices\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],  # Target attention projection layers\n",
    "    lora_dropout=0.05,  # Dropout for LoRA layers\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "\n",
    "# Load the tokenizer (again, with API key)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=\"YOUR_API_KEY\")\n",
    "\n",
    "# Set device to GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Print the number of trainable parameters (should be much less than full model)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Enable TensorFloat-32 (TF32) for faster training on supported GPUs\n",
    "if torch.cuda.is_available() and hasattr(torch, \"compile\"):\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# 2. Split English-translated QA pairs into training and test sets\n",
    "# Training set (90%): for LoRA fine-tuning\n",
    "# Test set (10%): for evaluation\n",
    "\n",
    "with open(\"qa_pairs_en.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "prompts, labels = [], []\n",
    "for sample in raw_data:\n",
    "    # Format each question and answer for supervised fine-tuning\n",
    "    prompts.append(f\"Question: {sample['question']}\\n\")\n",
    "    labels.append(\" \" + str(sample[\"answer\"]))\n",
    "\n",
    "# Split into train and test sets (90% train, 10% test)\n",
    "train_p, test_p, train_l, test_l = train_test_split(prompts, labels, test_size=0.1, random_state=42)\n",
    "train_dataset = Dataset.from_dict({\"prompt\": train_p, \"completion\": train_l})\n",
    "\n",
    "# 3. Initialize Tokenizer and tokenize the sentences\n",
    "\n",
    "# Load tokenizer again (with auth token for access)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=True)\n",
    "# Ensure pad token is set (required for some models)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "MAX_LEN = 256  # Maximum sequence length\n",
    "\n",
    "def tokenize_fn(example):\n",
    "    # Concatenate prompt and completion for full input\n",
    "    full_text = example[\"prompt\"] + example[\"completion\"]\n",
    "    # Tokenize the full text\n",
    "    tok_full = tokenizer(full_text, truncation=True, max_length=MAX_LEN, padding=False)\n",
    "    input_ids = tok_full[\"input_ids\"]\n",
    "    labels = input_ids.copy()\n",
    "    # Mask out the prompt part in the labels so loss is only computed on the answer\n",
    "    prompt_len = len(tokenizer(example[\"prompt\"], truncation=True, max_length=MAX_LEN)[\"input_ids\"])\n",
    "    for i in range(min(prompt_len, len(labels))):\n",
    "        labels[i] = -100  # -100 is ignored by the loss function\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": tok_full[\"attention_mask\"], \"labels\": labels}\n",
    "\n",
    "# Tokenize the training dataset\n",
    "train_tok = train_dataset.map(tokenize_fn, remove_columns=[\"prompt\", \"completion\"])\n",
    "\n",
    "# Data collator for dynamic padding and batch preparation\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"pt\", padding=\"longest\")\n",
    "\n",
    "# 4. Training arguments for LoRA fine-tuning\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"lora_rag\",  # Directory to save model checkpoints\n",
    "    num_train_epochs=2,     # Number of training epochs\n",
    "    per_device_train_batch_size=2,  # Batch size per device\n",
    "    gradient_accumulation_steps=1,  # Accumulate gradients over this many steps\n",
    "    learning_rate=1e-4,     # Learning rate\n",
    "    fp16=torch.cuda.is_available(), # Use mixed precision if on GPU\n",
    "    logging_steps=50,       # Log every 50 steps\n",
    "    save_strategy=\"epoch\",  # Save checkpoint at the end of each epoch\n",
    "    save_total_limit=2,     # Keep only the last 2 checkpoints\n",
    "    lr_scheduler_type=\"cosine\", # Use cosine learning rate scheduler\n",
    "    report_to=\"none\",       # Do not report to any tracking system\n",
    "    label_names=[],         # No custom label names\n",
    "    remove_unused_columns=False,  # Keep all columns in the dataset\n",
    ")\n",
    "\n",
    "# 5. Train the model and save the weights in \"lora_rag_weight\"\n",
    "\n",
    "# Initialize the Trainer for supervised fine-tuning\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model weights\n",
    "trainer.save_model(\"lora_rag_weight\")\n",
    "\n",
    "'''\n",
    "# Training Loss Results\n",
    "Step\tTraining Loss\n",
    "50\t3.748400\n",
    "100\t1.965000\n",
    "150\t2.850100\n",
    "200\t1.830300\n",
    "250\t0.645100\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b97dea6",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb6ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code assumes that 100 UpToDate PDF articles (focused on \"Infectious Diseases\" and relevant topics)\n",
    "# have already been downloaded and placed in the \"rag/\" folder.\n",
    "\n",
    "rag_folder = \"rag/\"  # Directory containing the PDF files\n",
    "pdf_files = glob.glob(os.path.join(rag_folder, \"*.pdf\"))  # List all PDF files in the folder\n",
    "records = []  # This will store the extracted data from each PDF\n",
    "\n",
    "# Iterate through each PDF file and extract its text content\n",
    "for pdf_path in tqdm(pdf_files, desc=\"Parsing PDFs\"):\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)  # Initialize the PDF reader\n",
    "        text = \"\"\n",
    "        # Extract text from each page in the PDF\n",
    "        for page in reader.pages:\n",
    "            # Some pages may not have extractable text, so use empty string as fallback\n",
    "            text += page.extract_text() or \"\"\n",
    "        text = text.strip()  # Remove leading/trailing whitespace\n",
    "        if text:\n",
    "            # Store the filename and extracted text if text is not empty\n",
    "            records.append({\n",
    "                \"title\": os.path.basename(pdf_path),\n",
    "                \"main_text\": text\n",
    "            })\n",
    "    except Exception as e:\n",
    "        # Print an error message if parsing fails for a file\n",
    "        print(f\"Failed to parse {pdf_path}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the extracted records with columns \"title\" and \"main_text\"\n",
    "df = pd.DataFrame(records, columns=[\"title\",\"main_text\"])\n",
    "# Save the DataFrame to a CSV file for later use\n",
    "df.to_csv('review_articles.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e04bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('review_articles.csv')  # Load the extracted article texts from CSV\n",
    "\n",
    "# Split the reference articles into chunks of max length 500 with overlap of 10 words\n",
    "all_chunks = []\n",
    "for example in tqdm(df['main_text']):  # Iterate through each article's main text\n",
    "    for c in chunk_text(example, max_length=500, overlap=10):  # Chunk the text for RAG\n",
    "        all_chunks.append(c)\n",
    "print(f\"chunk total: {len(all_chunks)}\")  # Print total number of chunks created\n",
    "\n",
    "# Use MedCPT as embedder (dimension: 768) to create FAISS index for RAG system\n",
    "\n",
    "MODEL_NAME = \"ncbi/MedCPT-Query-Encoder\"\n",
    "medcpt_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)  # Load tokenizer\n",
    "medcpt_model = AutoModel.from_pretrained(MODEL_NAME)          # Load model\n",
    "medcpt_model.eval()                                           # Set model to evaluation mode\n",
    "medcpt_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "medcpt_model = medcpt_model.to(medcpt_device)                 # Move model to device\n",
    "BATCH_SIZE = 16  # Number of chunks to process at once\n",
    "\n",
    "# Determine the embedding dimension by running a dummy input through the model\n",
    "with torch.no_grad():\n",
    "    dummy = medcpt_tokenizer(\"dummy\", return_tensors=\"pt\", max_length=64, truncation=True)\n",
    "    dummy = {k: v.to(medcpt_device) for k, v in dummy.items()}\n",
    "    dummy_emb = medcpt_model(**dummy).last_hidden_state[:, 0, :]  # Use [CLS] token embedding\n",
    "    embedding_dim = dummy_emb.size(-1)  # Should be 768 for MedCPT\n",
    "\n",
    "# Initialize a FAISS index for fast similarity search using L2 distance\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "chunk_embeddings_list = []\n",
    "\n",
    "# Batch process all chunks to generate embeddings and add to FAISS index\n",
    "for i in tqdm(range(0, len(all_chunks), BATCH_SIZE)):\n",
    "    batch_chunks = all_chunks[i:i+BATCH_SIZE]\n",
    "    with torch.no_grad():\n",
    "        encoded = medcpt_tokenizer(\n",
    "            batch_chunks,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors='pt',\n",
    "            max_length=512  # Truncate/pad to model's max input length\n",
    "        )\n",
    "        encoded = {k: v.to(medcpt_device) for k, v in encoded.items()}\n",
    "        emb = medcpt_model(**encoded).last_hidden_state[:, 0, :]  # Take [CLS] token embedding\n",
    "        emb = emb.cpu().numpy().astype(\"float32\")  # Convert to numpy for FAISS\n",
    "        chunk_embeddings_list.append(emb)\n",
    "chunk_embeddings = np.concatenate(chunk_embeddings_list, axis=0)  # Combine all embeddings\n",
    "index.add(chunk_embeddings)  # Add embeddings to FAISS index\n",
    "\n",
    "# Save the FAISS index to \"faiss_index.index\" for later retrieval\n",
    "faiss.write_index(index, \"faiss_index.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5715cf",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c196afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()  # Clear GPU memory cache to avoid OOM errors\n",
    "\n",
    "# Load the FAISS index for fast similarity search over document embeddings\n",
    "index = faiss.read_index(\"faiss_index.index\")\n",
    "\n",
    "# Load the reference articles (review articles) from CSV\n",
    "df = pd.read_csv('review_articles.csv')\n",
    "\n",
    "# Split all articles into overlapping chunks for retrieval\n",
    "all_chunks = []\n",
    "for example in tqdm(df['main_text']):\n",
    "    for c in chunk_text(example, max_length=500, overlap=50):\n",
    "        all_chunks.append(c)\n",
    "\n",
    "# Load MedCPT model and tokenizer for embedding queries and chunks\n",
    "MEDCPT_MODEL_NAME = \"ncbi/MedCPT-Query-Encoder\"\n",
    "medcpt_tokenizer = AutoTokenizer.from_pretrained(MEDCPT_MODEL_NAME)\n",
    "medcpt_model = AutoModel.from_pretrained(MEDCPT_MODEL_NAME)\n",
    "medcpt_model.eval()  # Set model to evaluation mode (no gradients)\n",
    "medcpt_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "medcpt_model = medcpt_model.to(medcpt_device)\n",
    "\n",
    "# Quantize the LLM to 4-bit to reduce memory usage (using bitsandbytes)\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,            \n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",            \n",
    "    quantization_config=bnb_cfg,\n",
    "    token = \"YOUR_API_KEY\", # Replace with your own HuggingFaceAPI key\n",
    ")\n",
    "\n",
    "# Load LoRA fine-tuned weights for the LLM\n",
    "model = PeftModel.from_pretrained(model, \"lora_rag_weight\")\n",
    "\n",
    "# Load tokenizer for the LLM (with API key if needed)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME,\n",
    "                                         token = \"YOUR_API_KEY\") # Replace with your own HuggingFaceAPI key\n",
    "\n",
    "# Move model to device and set to eval mode\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Ensure tokenizer has a pad token (required for generation)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def medcpt_embed(texts):\n",
    "    \"\"\"\n",
    "    Embed a list of texts using MedCPT model.\n",
    "    Returns a numpy array of embeddings.\n",
    "    \"\"\"\n",
    "    embs = []\n",
    "    with torch.no_grad():\n",
    "        for t in texts:\n",
    "            encoded = medcpt_tokenizer(\n",
    "                t,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                return_tensors='pt',\n",
    "                max_length=512,\n",
    "            )\n",
    "            encoded = {k: v.to(medcpt_device) for k, v in encoded.items()}\n",
    "            # Use [CLS] token embedding as sentence representation\n",
    "            emb = medcpt_model(**encoded).last_hidden_state[:, 0, :].cpu().numpy().astype(\"float32\")[0]\n",
    "            embs.append(emb)\n",
    "    embs = np.stack(embs)\n",
    "    return embs\n",
    "\n",
    "class AdvancedRetriever:\n",
    "    \"\"\"\n",
    "    AdvancedRetriever performs two-stage retrieval:\n",
    "    1. Dense retrieval using MedCPT + FAISS (fast, approximate)\n",
    "    2. Cross-encoder re-ranking (pointwise, more accurate)\n",
    "    Optionally applies MMR (Maximal Marginal Relevance) for diversity.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        faiss_index: faiss.Index,\n",
    "        all_chunks: List[str],\n",
    "        embed_fn,                         \n",
    "        cross_encoder_name: str = \"cross-encoder/ms-marco-MiniLM-L6-v2\",\n",
    "        device: str | None = None,\n",
    "    ):\n",
    "        self.index        = faiss_index\n",
    "        self.all_chunks   = all_chunks\n",
    "        self.embed_fn     = embed_fn\n",
    "        # Load cross-encoder for re-ranking (can be on CPU or GPU)\n",
    "        self.cross_enc    = CrossEncoder(\n",
    "            cross_encoder_name,\n",
    "            device=device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        )\n",
    "    def retrieve(\n",
    "        self,\n",
    "        query: str,\n",
    "        recall_k: int = 50,\n",
    "        rerank_k: int = 8,\n",
    "        use_mmr: bool = True,\n",
    "        lambda_mmr: float = 0.6,\n",
    "    ) -> Tuple[List[str], List[int]]:\n",
    "        \"\"\"\n",
    "        Retrieve top-k relevant chunks for a query.\n",
    "        Returns:\n",
    "            texts   – final context strings\n",
    "            indices – their original indices in `all_chunks`\n",
    "        \"\"\"\n",
    "        # 1️⃣ Dense retrieval: get top recall_k candidates from FAISS\n",
    "        q_emb = self.embed_fn([query]).astype(\"float32\")\n",
    "        _, I  = self.index.search(q_emb, recall_k)\n",
    "        cand_idx   = I[0].tolist()\n",
    "        cand_texts = [self.all_chunks[i] for i in cand_idx]\n",
    "        # 2️⃣ Cross-encoder re-ranking: score each candidate with the query\n",
    "        pairs       = list(zip([query] * len(cand_texts), cand_texts))\n",
    "        ce_scores   = self.cross_enc.predict(pairs, convert_to_numpy=True)\n",
    "        ranked      = sorted(\n",
    "            zip(cand_idx, cand_texts, ce_scores),\n",
    "            key=lambda x: x[2],\n",
    "            reverse=True\n",
    "        )[: rerank_k]\n",
    "        idxs, texts, _ = zip(*ranked)\n",
    "        # 3️⃣ Optional: MMR diversification to reduce redundancy\n",
    "        if use_mmr and rerank_k > 1:\n",
    "            embs      = self.embed_fn(list(texts))\n",
    "            order     = self._mmr(embs, q_emb[0], k=rerank_k, lamb=lambda_mmr)\n",
    "            idxs      = [idxs[i]  for i in order]\n",
    "            texts     = [texts[i] for i in order]\n",
    "        return list(texts), list(idxs)\n",
    "    @staticmethod\n",
    "    def _mmr(doc_embs: np.ndarray, query_emb: np.ndarray,\n",
    "             k: int = 20, lamb: float = 0.6) -> List[int]:\n",
    "        \"\"\"\n",
    "        Maximal Marginal Relevance (MMR) for selecting a diverse set of top-k documents.\n",
    "        Returns indices of selected documents.\n",
    "        \"\"\"\n",
    "        selected, cand = [], list(range(len(doc_embs)))\n",
    "\n",
    "        # Compute relevance of each doc to the query\n",
    "        rel = skl_cosine(doc_embs, query_emb.reshape(1, -1)).flatten()\n",
    "\n",
    "        while len(selected) < min(k, len(doc_embs)):\n",
    "            if not selected:\n",
    "                sel = int(np.argmax(rel))\n",
    "            else:\n",
    "                # Compute redundancy (similarity to already selected docs)\n",
    "                redund = skl_cosine(\n",
    "                    doc_embs[cand],        \n",
    "                    doc_embs[selected]     \n",
    "                ).max(axis=1)\n",
    "                # MMR score: trade-off between relevance and redundancy\n",
    "                mmr = lamb * rel[cand] - (1 - lamb) * redund\n",
    "                sel = cand[int(np.argmax(mmr))]\n",
    "            selected.append(sel)\n",
    "            cand.remove(sel)\n",
    "        return selected\n",
    "\n",
    "class RAGGenerator:\n",
    "    \"\"\"\n",
    "    RAGGenerator generates answers by retrieving relevant context and prompting the LLM.\n",
    "    \"\"\"\n",
    "    def __init__(self, retriever: AdvancedRetriever, llm, tokenizer):\n",
    "        self.ret   = retriever\n",
    "        self.llm   = llm\n",
    "        self.tok   = tokenizer\n",
    "        # Ensure tokenizer has a pad token for generation\n",
    "        if self.tok.pad_token is None:\n",
    "            self.tok.pad_token = self.tok.eos_token\n",
    "    @torch.inference_mode()\n",
    "    def __call__(\n",
    "        self,\n",
    "        query: str,\n",
    "        recall_k: int = 50,\n",
    "        rerank_k: int = 8,\n",
    "        max_new_tokens: int = 128,\n",
    "        top_p: float = 0.4,\n",
    "        temperature: float = 0.1,\n",
    "    ) -> str:\n",
    "        # Retrieve relevant context chunks for the query\n",
    "        ctx_texts, _ = self.ret.retrieve(query, recall_k, rerank_k)\n",
    "        # Format the evidence for the prompt\n",
    "        evidence = \"\\n\\n\".join(f\"[Doc {i+1}]\\n{txt}\" for i, txt in enumerate(ctx_texts))\n",
    "        # Construct the prompt for the LLM\n",
    "        prompt   = (\n",
    "            \"@@@ Evidence\\n\"\n",
    "            f\"{evidence}\\n\\n\"\n",
    "            \"@@@ Instruction: Base your [@@@ Answer] ONLY on the evidence above. \"\n",
    "            \"Do NOT copy phrases verbatim and keep it under 200 characters.\\n\\n\"\n",
    "            f\"Question: {query}\\n@@@ Answer\"\n",
    "        )\n",
    "        # Tokenize the prompt and move to device\n",
    "        inputs = self.tok(\n",
    "            prompt, return_tensors=\"pt\", truncation=True, max_length=4096\n",
    "        ).to(self.llm.device)\n",
    "        # Generate the answer using the LLM\n",
    "        output = self.llm.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            eos_token_id=self.tok.eos_token_id,\n",
    "            pad_token_id=self.tok.eos_token_id,\n",
    "            no_repeat_ngram_size=3,\n",
    "            top_p=top_p,\n",
    "            temperature=temperature,\n",
    "            repetition_penalty=1.15,\n",
    "            do_sample=True,\n",
    "        )\n",
    "        # Decode and extract the answer from the output\n",
    "        answer = self.tok.decode(output[0], skip_special_tokens=True)\n",
    "        return answer.split(\"@@@ Answer\")[-1].strip()\n",
    "\n",
    "# Instantiate the advanced retriever with FAISS, chunks, and embedding function\n",
    "adv_retriever = AdvancedRetriever(\n",
    "    faiss_index=index,\n",
    "    all_chunks=all_chunks,\n",
    "    embed_fn=medcpt_embed,\n",
    ")\n",
    "\n",
    "# Initialize the RAG generator with retriever, LLM, and tokenizer\n",
    "rag_gen = RAGGenerator(\n",
    "    retriever=adv_retriever,\n",
    "    llm=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30133dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()  # Clear CUDA memory cache to prevent OOM errors\n",
    "\n",
    "# Load the QA pairs from the JSON file\n",
    "with open(\"qa_pairs_en.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "prompts, labels = [], []\n",
    "# Prepare prompts and labels for training/testing\n",
    "for sample in raw_data:\n",
    "    prompts.append(f\"Question: {sample['question']}\")\n",
    "    labels.append(\" \" + str(sample[\"answer\"]))\n",
    "\n",
    "# Split the data into training and test sets (10% test)\n",
    "train_p, test_p, train_l, test_l = train_test_split(prompts, labels, test_size=0.1, random_state=1)\n",
    "train_dataset = pd.DataFrame({\"prompt\": train_p, \"completion\": train_l})\n",
    "\n",
    "# Test the RAG + Fine-tuned model with the test set QA pairs\n",
    "# Compare the reference answer from the QA pairs with the generated answer from the RAG + Fine-tuned model\n",
    "\n",
    "gen_ans, ref_ans, qs = [], [], []\n",
    "for p, ref in tqdm(list(zip(test_p, test_l)), desc=\"Testing (RAG)\"):\n",
    "    ans = rag_gen(p)  # Generate answer using the RAG pipeline\n",
    "    gen_ans.append(ans)\n",
    "    try:\n",
    "        # Try to parse the reference answer as a dictionary and extract the last value if possible\n",
    "        ref_dict = eval(ref.strip())\n",
    "        if isinstance(ref_dict, dict):\n",
    "            last_key = list(ref_dict.keys())[-1]\n",
    "            last_value = ref_dict[last_key]\n",
    "            ref_ans.append(str(last_value))\n",
    "        else:\n",
    "            ref_ans.append(ref.strip())\n",
    "    except Exception:\n",
    "        # If parsing fails, just use the raw reference string\n",
    "        ref_ans.append(ref.strip())\n",
    "    qs.append(p)\n",
    "\n",
    "# Function to cut the generated answer to the last period for cleaner output\n",
    "def cut_to_last_period(text):\n",
    "    idx = text.rfind('.')\n",
    "    if idx != -1:\n",
    "        return text[:idx+1].strip()\n",
    "    else:\n",
    "        return text.strip()\n",
    "\n",
    "# Apply the cut_to_last_period function to all generated answers\n",
    "gen_ans = [cut_to_last_period(ans) for ans in gen_ans]\n",
    "\n",
    "# Embed the generated and reference answers using MedCPT embeddings\n",
    "gen_embs = medcpt_embed(gen_ans)\n",
    "ref_embs = medcpt_embed(ref_ans)\n",
    "\n",
    "# Calculate cosine similarity between generated and reference answers\n",
    "num   = (gen_embs * ref_embs).sum(axis=1)\n",
    "denom = np.linalg.norm(gen_embs, axis=1) * np.linalg.norm(ref_embs, axis=1)\n",
    "sim   = num / denom\n",
    "\n",
    "# Calculate BLEU and ROUGE-LCS F1 score for evaluation\n",
    "\n",
    "# Prepare data for BLEU calculation: BLEU expects tokenized references and hypotheses\n",
    "bleu_refs = [[ref.split()] for ref in ref_ans]\n",
    "bleu_hyps = [gen.split() for gen in gen_ans]\n",
    "bleu_score = corpus_bleu(bleu_refs, bleu_hyps, smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "# Initialize ROUGE scorer for ROUGE-L (Longest Common Subsequence) F1\n",
    "rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "rouge_l_f1s = []\n",
    "for ref, gen in zip(ref_ans, gen_ans):\n",
    "    scores = rouge.score(ref, gen)\n",
    "    rouge_l_f1s.append(scores['rougeL'].fmeasure)\n",
    "# Compute mean ROUGE-L F1 score\n",
    "rouge_l_f1 = sum(rouge_l_f1s) / len(rouge_l_f1s) if rouge_l_f1s else 0.0\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"\\nTest set mean cosine similarity: {np.mean(sim):.4f}\")\n",
    "print(f\"Test set BLEU: {bleu_score:.4f}\")\n",
    "print(f\"Test set ROUGE-LCS F1: {rouge_l_f1:.4f}\")\n",
    "\n",
    "# Print a few example QAs for inspection\n",
    "for i in range(min(3, len(qs))):\n",
    "    print(f\"\\nQ: {qs[i]}\\nReference: {ref_ans[i]}\\nGenerated: {gen_ans[i]}\\nCosine similarity: {sim[i]:.4f}\") \n",
    "\n",
    "'''\n",
    "<Example of Reply>\n",
    "Reference: (1) Antibiotics are recommended to be maintained as they are, and adjusted according to the results of culture. (2) Esophageal candidasis is also suspected, and fluconazole dosage is recommended to be changed to 400 mg on day 1, then 200 to 400 mg once daily as a normal new functional baseline. Currently, after application of PTGBD, a reduction in inflammatory markers and a fever spike is not observed, but a suppressed RUQ tenderness is observed.\n",
    "Generated: : #. There is no specific recommendation for empirical antibiotic therapy because there is limited information about the previous medical condition and current status of the patient who has already recovered from the asymptomatic stage of the past infection until now, but only empirically administered vancomycin and meropenem were given up to the time of confirmation of the BAL culture result, so please continue administration of vanco + mero according to the doctor's discretion until the recovery of the lung function is completed. If you want to change the medication, we recommend confirming the susceptibility test of the cultured bacteria before changing the drug.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "240305",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
