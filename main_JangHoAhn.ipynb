{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1669ff4",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bbc33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ( \n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    AutoModel,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AutoModelForSeq2SeqLM\n",
    ")\n",
    "import polars as pl\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import glob, os\n",
    "from PyPDF2 import PdfReader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch._dynamo\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import faiss\n",
    "import re\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel, prepare_model_for_kbit_training\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from pathlib import Path\n",
    "from sentence_transformers import CrossEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity as skl_cosine\n",
    "from typing import List, Tuple\n",
    "\n",
    "def chunk_text(text, max_length=500, overlap=50):\n",
    "    sentences = text.split(\". \")\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for sent in sentences:\n",
    "        if len(current_chunk) + len(sent) + 2 <= max_length:\n",
    "            current_chunk += sent + \". \"\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            overlap_words = current_chunk.split()[-overlap:]\n",
    "            current_chunk = \" \".join(overlap_words) + \" \" + sent + \". \"\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    a_norm = a / (np.linalg.norm(a) + 1e-8)\n",
    "    b_norm = b / (np.linalg.norm(b) + 1e-8)\n",
    "    return float(np.dot(a_norm, b_norm))\n",
    "    \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "MODEL_NAME =\"Intelligent-Internet/II-Medical-8B-1706\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb33663",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac56c6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "doc_type = pl.read_excel('datasets_folder/SNUHNOTE/1.0.1/document_type_level_mst.xlsx')\n",
    "doc_type = doc_type.filter(\n",
    "    (pl.col(\"doc_type_id\") == \"D008\") & \n",
    "    (pl.col(\"mdfm_name\").str.contains(\"감염\"))\n",
    ")\n",
    "# mdfm_id = 41813, 41814, 42345 for 타과의뢰회신\n",
    "'''\n",
    "dfs = []\n",
    "for i in tqdm(range(1, 8)):\n",
    "    df = pl.read_excel(f'datasets_folder/SNUHNOTE/1.0.1/4_DGNS/DGNS_{i}.xlsx')\n",
    "    df1 = df.filter(\n",
    "        ((pl.col(\"level_path\").str.contains(\"41813\"))|(pl.col(\"level_path\").str.contains(\"41814\"))|(pl.col(\"level_path\").str.contains(\"42345\"))) &\n",
    "        ~(pl.col(\"content\")=='승인하였습니다.\\n') & ~(pl.col(\"content\")=='승인하였습니다. \\n') &  ~(pl.col(\"content\")=='승인하였습니다. \\n\\n') & ~(pl.col(\"content\")=='승인하였습니다.\\n\\n') &\n",
    "        ~(pl.col(\"content\")=='승인하였습니다.\\n\\n\\n') & ~(pl.col(\"content\")=='승인하였습니다. \\n\\n\\n') &\n",
    "        ~(pl.col(\"content\")=='승인하겠습니다.\\n') & ~(pl.col(\"content\")=='승인하겠습니다. \\n') &  ~(pl.col(\"content\")=='승인하겠습니다. \\n\\n') & ~(pl.col(\"content\")=='승인하겠습니다.\\n\\n') &\n",
    "        ~(pl.col(\"content\")=='승인하겠습니다.\\n\\n\\n') & ~(pl.col(\"content\")=='승인하겠습니다. \\n\\n\\n') &\n",
    "        ~(pl.col(\"content\")=='투약 승인하였습니다.\\n') & ~(pl.col(\"content\")=='투약 승인하였습니다. \\n') & ~(pl.col(\"content\")=='투약 승인하였습니다. \\n\\n') &~(pl.col(\"content\")=='투약 승인하였습니다.\\n\\n') &\n",
    "        ~(pl.col(\"content\").str.contains(\"문의\")) & ~(pl.col(\"content\").str.contains(\"의뢰\")) &~(pl.col(\"content\").str.contains(\"취소된 처방\")) &\n",
    "        ((pl.col(\"content\").str.contains(\"추천\"))|(pl.col(\"content\").str.contains(\"권장\"))|(pl.col(\"content\").str.contains(\"승인\"))|(pl.col(\"content\").str.contains(\"고려\"))|\n",
    "        (pl.col(\"content\").str.contains(\"니다\")|(pl.col(\"content\").str.contains(\"권고\"))))\n",
    "    )\n",
    "    df1 = df1.sort([\"nid\", \"rec_dt_offset\"])\n",
    "    df1 = df1.group_by(\"nid\").agg([\n",
    "        pl.col(\"rec_dt_offset\").first().alias(\"rec_dt_offset\"),\n",
    "        pl.col(\"level_path\").first().alias(\"level_path\"),\n",
    "        pl.col(\"content\").implode().alias(\"content_list\")\n",
    "    ])\n",
    "    df1 = df1.with_columns(\n",
    "        pl.col(\"content_list\").list.join(\" \").alias(\"content\")\n",
    "    ).select([\"nid\", \"rec_dt_offset\", \"content\"])\n",
    "\n",
    "    df2 = df.filter(\n",
    "        ((pl.col(\"level_path\").str.contains(\"41813\"))|(pl.col(\"level_path\").str.contains(\"41814\"))|(pl.col(\"level_path\").str.contains(\"42345\"))) &\n",
    "        ((pl.col(\"content\").str.contains(\"문의\"))|(pl.col(\"content\").str.contains(\"상의\"))|(pl.col(\"content\").str.contains(\"의뢰\"))) &\n",
    "        ~((pl.col(\"content\").str.contains(\"추천\"))|(pl.col(\"content\").str.contains(\"권장\"))|(pl.col(\"content\").str.contains(\"승인\"))|(pl.col(\"content\").str.contains(\"고려\"))|\n",
    "        (pl.col(\"content\").str.contains(\"바랍니다\"))|(pl.col(\"content\").str.contains(\"의뢰주셔서\"))|(pl.col(\"content\").str.contains(\"의뢰 감사\"))|(pl.col(\"content\").str.contains(\"권고\"))|\n",
    "        (pl.col(\"content\").str.contains(\"분 이내\"))|(pl.col(\"content\").str.contains(\"해당없음\"))|(pl.col(\"content\").str.contains(\"니다\")) )\n",
    "    )\n",
    "\n",
    "    df2 = df2.sort([\"nid\", \"rec_dt_offset\"])\n",
    "    df2 = df2.group_by(\"nid\").agg([\n",
    "        pl.col(\"rec_dt_offset\").alias(\"rec_dt_offset_list\"),\n",
    "        pl.col(\"content\").alias(\"content_list\")\n",
    "    ])\n",
    "    \n",
    "    df2_long = (\n",
    "        df2\n",
    "        .select([\"nid\", \"rec_dt_offset_list\", \"content_list\"])\n",
    "        .explode([\"rec_dt_offset_list\", \"content_list\"])\n",
    "        .rename({\"rec_dt_offset_list\": \"df2_rec_dt_offset\",\n",
    "                 \"content_list\":        \"cst\"})          \n",
    "        .sort([\"nid\", \"df2_rec_dt_offset\"])\n",
    "    )\n",
    "\n",
    "    final_df = (\n",
    "        df1.sort([\"nid\", \"rec_dt_offset\"])               \n",
    "           .join_asof(                                   \n",
    "                df2_long,\n",
    "                left_on=\"rec_dt_offset\",\n",
    "                right_on=\"df2_rec_dt_offset\",\n",
    "                by=\"nid\",\n",
    "                strategy=\"backward\"                      \n",
    "           )\n",
    "           .select([\"nid\", \"rec_dt_offset\", \"content\", \"cst\"])\n",
    "    )\n",
    "    dfs.append(final_df)\n",
    "\n",
    "x1 = pl.concat(dfs)\n",
    "x1 = x1.sort(\"rec_dt_offset\").group_by(\"nid\").first()\n",
    "x1 = x1.filter(pl.col(\"cst\").is_not_null())\n",
    "def remove_im_to_baesang(text):\n",
    "    return re.sub(r'IM((?:(?!배상).)*?)배상', '', text, flags=re.DOTALL)\n",
    "x1 = x1.with_columns(\n",
    "    pl.col('content').map_elements(remove_im_to_baesang).alias('content')\n",
    ")\n",
    "\n",
    "qa_pairs = []\n",
    "for row in tqdm(x1.iter_rows(named=True), total=x1.height, desc=\"Generating QA pairs\"):  # progress bar\n",
    "    question = (\n",
    "        \"@@@ Task: \\n\"\n",
    "        \"You are an infectious-disease consultant.  Compose ONE brief consultation note as the [@@@ Answer] that obeys **all** rules 1~4 below:\\n\"\n",
    "        \"1. Respond including 3 categories below:\\n\"\n",
    "        \"   • (1) suspected / confirmed pathogen (if any)\\n\"\n",
    "        \"   • (2) recommended medications (if any)\\n\"\n",
    "        \"   • (3) additional labs / cultures (if any)\\n\"\n",
    "        \"2. You must NOT repeat any phrase or idea in each sentence.\\n\"\n",
    "        \"3. Do not copy ANY text from [@@@ Attending physician’s message].\\n\"\n",
    "        \"4. Write in complete sentences and ALWAYS finish each sentence with a period.\\n\\n\"\n",
    "        \"@@@ Attending physician’s message\\n\"\n",
    "        f\"{row['cst']}\\n\\n\"\n",
    "        \"@@@ Answer:\"\n",
    "    )\n",
    "    answer = row[\"content\"]\n",
    "    qa_pairs.append({\n",
    "        \"question\": question,\n",
    "        \"answer\": answer\n",
    "    })\n",
    "with open(\"qa_pairs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(qa_pairs, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b993b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"facebook/nllb-200-3.3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, src_lang=\"kor_Hang\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "def translate_ko2en(text: str, max_len: int = 512, num_beams: int = 6) -> str:\n",
    "    \"\"\"한국어 → 영어 단일 문장/문단 번역.\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_len,\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"eng_Latn\"), \n",
    "            max_length=max_len,\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    \n",
    "SRC_JSON = Path(\"qa_pairs.json\")\n",
    "DST_JSON = Path(\"qa_pairs_en.json\")\n",
    "\n",
    "with SRC_JSON.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "translated = []\n",
    "for item in tqdm(data, desc=\"Translating 'question' and 'answer' fields\"):\n",
    "    new_item = item.copy()  # 원본 유지\n",
    "    if isinstance(item.get(\"answer\"), str):\n",
    "        translated_answer = translate_ko2en(item[\"answer\"])\n",
    "        translated_answer = re.sub(r'(?<=\\S)investment(?=\\S)', 'administration', translated_answer, flags=re.IGNORECASE)\n",
    "        new_item[\"answer\"] = translated_answer\n",
    "    translated.append(new_item)\n",
    "\n",
    "with DST_JSON.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Saved translated data ➜ {DST_JSON.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7d10db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. LoRA 설정 --------------------------------------------------------------\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,            # <- real 4-bit weights (nf4)\n",
    "    bnb_4bit_quant_type=\"nf4\",    # Normal-Float-4 (best accuracy)\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",            \n",
    "    quantization_config=bnb_cfg,\n",
    "    token = \"YOUR_API_KEY\"\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],  # change to model’s names\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME,\n",
    "                                         token = \"YOUR_API_KEY\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.print_trainable_parameters()\n",
    "if torch.cuda.is_available() and hasattr(torch, \"compile\"):\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "# 2. 데이터 로드 및 분할 -------------------------------------------------------\n",
    "with open(\"qa_pairs_en.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "prompts, labels = [], []\n",
    "for sample in raw_data:\n",
    "    prompts.append(f\"Question: {sample['question']}\\n\")\n",
    "    labels.append(\" \" + str(sample[\"answer\"]))\n",
    "train_p, test_p, train_l, test_l = train_test_split(prompts, labels, test_size=0.1, random_state=42)\n",
    "train_dataset = Dataset.from_dict({\"prompt\": train_p, \"completion\": train_l})\n",
    "# 3. 토크나이저 --------------------------------------------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "# 4. Tokenize (tip 2) --------------------------------------------------------\n",
    "MAX_LEN = 256\n",
    "def tokenize_fn(example):\n",
    "    full_text = example[\"prompt\"] + example[\"completion\"]\n",
    "    tok_full = tokenizer(full_text, truncation=True, max_length=MAX_LEN, padding=False)\n",
    "    input_ids = tok_full[\"input_ids\"]\n",
    "    labels = input_ids.copy()\n",
    "    prompt_len = len(tokenizer(example[\"prompt\"], truncation=True, max_length=MAX_LEN)[\"input_ids\"])\n",
    "    for i in range(min(prompt_len, len(labels))):\n",
    "        labels[i] = -100\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": tok_full[\"attention_mask\"], \"labels\": labels}\n",
    "train_tok = train_dataset.map(tokenize_fn, remove_columns=[\"prompt\", \"completion\"])\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"pt\", padding=\"longest\")\n",
    "# 5. Training arguments (tips 4 & 6) ----------------------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"lora_rag\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=2,  \n",
    "    gradient_accumulation_steps=1,  \n",
    "    learning_rate=1e-4,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",  # checkpoint once per epoch\n",
    "    save_total_limit=2,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"none\",\n",
    "    label_names=[],\n",
    "    remove_unused_columns=False,  \n",
    ")\n",
    "# 6. Trainer -----------------------------------------------------------------\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=train_tok, data_collator=data_collator)\n",
    "trainer.train()\n",
    "trainer.save_model(\"lora_rag_weight\")\n",
    "\n",
    "'''\n",
    "Step\tTraining Loss\n",
    "50\t3.748400\n",
    "100\t1.965000\n",
    "150\t2.850100\n",
    "200\t1.830300\n",
    "250\t0.645100\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b97dea6",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb6ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_folder = \"rag/\"\n",
    "pdf_files = glob.glob(os.path.join(rag_folder, \"*.pdf\"))\n",
    "records = []\n",
    "\n",
    "for pdf_path in tqdm(pdf_files, desc=\"Parsing PDFs\"):\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "        text = text.strip()\n",
    "        if text:\n",
    "            records.append({\n",
    "                \"title\": os.path.basename(pdf_path),\n",
    "                \"main_text\": text\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse {pdf_path}: {e}\")\n",
    "\n",
    "df = pd.DataFrame(records, columns=[\"title\",\"main_text\"])\n",
    "df.to_csv('review_articles.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e04bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('review_articles.csv')\n",
    "all_chunks = []\n",
    "for example in tqdm(df['main_text']):\n",
    "    for c in chunk_text(example, max_length=500, overlap=10):\n",
    "        all_chunks.append(c)\n",
    "print(f\"chunk total: {len(all_chunks)}\")\n",
    "\n",
    "# Use MedCPT as embedder\n",
    "MODEL_NAME = \"ncbi/MedCPT-Query-Encoder\"\n",
    "medcpt_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "medcpt_model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "medcpt_model.eval()\n",
    "medcpt_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "medcpt_model = medcpt_model.to(medcpt_device)\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "with torch.no_grad():\n",
    "    dummy = medcpt_tokenizer(\"dummy\", return_tensors=\"pt\", max_length=64, truncation=True)\n",
    "    dummy = {k: v.to(medcpt_device) for k, v in dummy.items()}\n",
    "    dummy_emb = medcpt_model(**dummy).last_hidden_state[:, 0, :]\n",
    "    embedding_dim = dummy_emb.size(-1)\n",
    "\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "chunk_embeddings_list = []\n",
    "for i in tqdm(range(0, len(all_chunks), BATCH_SIZE)):\n",
    "    batch_chunks = all_chunks[i:i+BATCH_SIZE]\n",
    "    with torch.no_grad():\n",
    "        encoded = medcpt_tokenizer(\n",
    "            batch_chunks,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors='pt',\n",
    "            max_length= 512\n",
    "        )\n",
    "        encoded = {k: v.to(medcpt_device) for k, v in encoded.items()}\n",
    "        emb = medcpt_model(**encoded).last_hidden_state[:, 0, :]  # [BATCH_SIZE, 768]\n",
    "        emb = emb.cpu().numpy().astype(\"float32\")\n",
    "        chunk_embeddings_list.append(emb)\n",
    "chunk_embeddings = np.concatenate(chunk_embeddings_list, axis=0)\n",
    "index.add(chunk_embeddings)\n",
    "\n",
    "faiss.write_index(index, \"faiss_index.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5715cf",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c196afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "index = faiss.read_index(\"faiss_index.index\")\n",
    "df = pd.read_csv('review_articles.csv')\n",
    "all_chunks = []\n",
    "for example in tqdm(df['main_text']):\n",
    "    for c in chunk_text(example, max_length=500, overlap=50):\n",
    "        all_chunks.append(c)\n",
    "\n",
    "MEDCPT_MODEL_NAME = \"ncbi/MedCPT-Query-Encoder\"\n",
    "medcpt_tokenizer = AutoTokenizer.from_pretrained(MEDCPT_MODEL_NAME)\n",
    "medcpt_model = AutoModel.from_pretrained(MEDCPT_MODEL_NAME)\n",
    "medcpt_model.eval()\n",
    "medcpt_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "medcpt_model = medcpt_model.to(medcpt_device)\n",
    "\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,            # <- real 4-bit weights (nf4)\n",
    "    bnb_4bit_quant_type=\"nf4\",    # Normal-Float-4 (best accuracy)\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",            \n",
    "    quantization_config=bnb_cfg,\n",
    "    token = \"YOUR_API_KEY\",\n",
    "    \n",
    ")\n",
    "model = PeftModel.from_pretrained(model, \"lora_rag_weight\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME,\n",
    "                                         token = \"YOUR_API_KEY\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def medcpt_embed(texts):\n",
    "    embs = []\n",
    "    with torch.no_grad():\n",
    "        for t in texts:\n",
    "            encoded = medcpt_tokenizer(\n",
    "                t,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                return_tensors='pt',\n",
    "                max_length=512,\n",
    "            )\n",
    "            encoded = {k: v.to(medcpt_device) for k, v in encoded.items()}\n",
    "            emb = medcpt_model(**encoded).last_hidden_state[:, 0, :].cpu().numpy().astype(\"float32\")[0]\n",
    "            embs.append(emb)\n",
    "    embs = np.stack(embs)\n",
    "    return embs\n",
    "\n",
    "class AdvancedRetriever:\n",
    "    \"\"\"\n",
    "    • Stage-1  : dense search with MedCPT + FAISS\n",
    "    • Stage-2  : Cross-Encoder re-ranking (point-wise)\n",
    "    • Optional : MMR diversification\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        faiss_index: faiss.Index,\n",
    "        all_chunks: List[str],\n",
    "        embed_fn,                         \n",
    "        cross_encoder_name: str = \"cross-encoder/ms-marco-MiniLM-L6-v2\",\n",
    "        device: str | None = None,\n",
    "    ):\n",
    "        self.index        = faiss_index\n",
    "        self.all_chunks   = all_chunks\n",
    "        self.embed_fn     = embed_fn\n",
    "        self.cross_enc    = CrossEncoder(\n",
    "            cross_encoder_name,\n",
    "            device=device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        )\n",
    "    def retrieve(\n",
    "        self,\n",
    "        query: str,\n",
    "        recall_k: int = 50,\n",
    "        rerank_k: int = 8,\n",
    "        use_mmr: bool = True,\n",
    "        lambda_mmr: float = 0.6,\n",
    "    ) -> Tuple[List[str], List[int]]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            texts   – final context strings\n",
    "            indices – their original indices in `all_chunks`\n",
    "        \"\"\"\n",
    "        # 1️⃣\n",
    "        q_emb = self.embed_fn([query]).astype(\"float32\")\n",
    "        _, I  = self.index.search(q_emb, recall_k)\n",
    "        cand_idx   = I[0].tolist()\n",
    "        cand_texts = [self.all_chunks[i] for i in cand_idx]\n",
    "        # 2️⃣\n",
    "        pairs       = list(zip([query] * len(cand_texts), cand_texts))\n",
    "        ce_scores   = self.cross_enc.predict(pairs, convert_to_numpy=True)\n",
    "        ranked      = sorted(\n",
    "            zip(cand_idx, cand_texts, ce_scores),\n",
    "            key=lambda x: x[2],\n",
    "            reverse=True\n",
    "        )[: rerank_k]\n",
    "        idxs, texts, _ = zip(*ranked)\n",
    "        # 3️⃣\n",
    "        if use_mmr and rerank_k > 1:\n",
    "            embs      = self.embed_fn(list(texts))\n",
    "            order     = self._mmr(embs, q_emb[0], k=rerank_k, lamb=lambda_mmr)\n",
    "            idxs      = [idxs[i]  for i in order]\n",
    "            texts     = [texts[i] for i in order]\n",
    "        return list(texts), list(idxs)\n",
    "    @staticmethod\n",
    "    def _mmr(doc_embs: np.ndarray, query_emb: np.ndarray,\n",
    "             k: int = 20, lamb: float = 0.6) -> List[int]: # Selecting Top-K\n",
    "        \"\"\"\n",
    "        Maximal Marginal Relevance.\n",
    "        \"\"\"\n",
    "        selected, cand = [], list(range(len(doc_embs)))\n",
    "\n",
    "        # relevance of each doc to the query\n",
    "        rel = skl_cosine(doc_embs, query_emb.reshape(1, -1)).flatten()\n",
    "\n",
    "        while len(selected) < min(k, len(doc_embs)):\n",
    "            if not selected:\n",
    "                sel = int(np.argmax(rel))\n",
    "            else:\n",
    "                redund = skl_cosine(\n",
    "                    doc_embs[cand],        \n",
    "                    doc_embs[selected]     \n",
    "                ).max(axis=1)\n",
    "                mmr = lamb * rel[cand] - (1 - lamb) * redund\n",
    "                sel = cand[int(np.argmax(mmr))]\n",
    "            selected.append(sel)\n",
    "            cand.remove(sel)\n",
    "        return selected\n",
    "\n",
    "class RAGGenerator:\n",
    "    def __init__(self, retriever: AdvancedRetriever, llm, tokenizer):\n",
    "        self.ret   = retriever\n",
    "        self.llm   = llm\n",
    "        self.tok   = tokenizer\n",
    "        if self.tok.pad_token is None:\n",
    "            self.tok.pad_token = self.tok.eos_token\n",
    "    @torch.inference_mode()\n",
    "    def __call__(\n",
    "        self,\n",
    "        query: str,\n",
    "        recall_k: int = 50,\n",
    "        rerank_k: int = 8,\n",
    "        max_new_tokens: int = 128,\n",
    "        top_p: float = 0.4,\n",
    "        temperature: float = 0.1,\n",
    "    ) -> str:\n",
    "        ctx_texts, _ = self.ret.retrieve(query, recall_k, rerank_k)\n",
    "        evidence = \"\\n\\n\".join(f\"[Doc {i+1}]\\n{txt}\" for i, txt in enumerate(ctx_texts))\n",
    "        prompt   = (\n",
    "            \"@@@ Evidence\\n\"\n",
    "            f\"{evidence}\\n\\n\"\n",
    "            \"@@@ Instruction: Base your [@@@ Answer] ONLY on the evidence above. \"\n",
    "            \"Do NOT copy phrases verbatim and keep it under 200 characters.\\n\\n\"\n",
    "            f\"Question: {query}\\n@@@ Answer\"\n",
    "        )\n",
    "        inputs = self.tok(\n",
    "            prompt, return_tensors=\"pt\", truncation=True, max_length=4096\n",
    "        ).to(self.llm.device)\n",
    "        output = self.llm.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            eos_token_id=self.tok.eos_token_id,\n",
    "            pad_token_id=self.tok.eos_token_id,\n",
    "            no_repeat_ngram_size=3,\n",
    "            top_p=top_p,\n",
    "            temperature=temperature,\n",
    "            repetition_penalty=1.15,\n",
    "            do_sample=True,\n",
    "        )\n",
    "        answer = self.tok.decode(output[0], skip_special_tokens=True)\n",
    "        return answer.split(\"@@@ Answer\")[-1].strip()\n",
    "\n",
    "adv_retriever = AdvancedRetriever(\n",
    "    faiss_index=index,\n",
    "    all_chunks=all_chunks,\n",
    "    embed_fn=medcpt_embed,\n",
    ")\n",
    "\n",
    "rag_gen = RAGGenerator(\n",
    "    retriever=adv_retriever,\n",
    "    llm=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30133dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "with open(\"qa_pairs_en.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "prompts, labels = [], []\n",
    "for sample in raw_data:\n",
    "    prompts.append(f\"Question: {sample['question']}\")\n",
    "    labels.append(\" \" + str(sample[\"answer\"]))\n",
    "train_p, test_p, train_l, test_l = train_test_split(prompts, labels, test_size=0.1, random_state=1)\n",
    "train_dataset = pd.DataFrame({\"prompt\": train_p, \"completion\": train_l})\n",
    "\n",
    "gen_ans, ref_ans, qs = [], [], []\n",
    "for p, ref in tqdm(list(zip(test_p, test_l)), desc=\"Testing (RAG)\"):\n",
    "    ans =rag_gen(p)\n",
    "    gen_ans.append(ans)\n",
    "    try:\n",
    "        ref_dict = eval(ref.strip())\n",
    "        if isinstance(ref_dict, dict):\n",
    "            last_key = list(ref_dict.keys())[-1]\n",
    "            last_value = ref_dict[last_key]\n",
    "            ref_ans.append(str(last_value))\n",
    "        else:\n",
    "            ref_ans.append(ref.strip())\n",
    "    except Exception:\n",
    "        ref_ans.append(ref.strip())\n",
    "    qs.append(p)\n",
    "\n",
    "def cut_to_last_period(text):\n",
    "    idx = text.rfind('.')\n",
    "    if idx != -1:\n",
    "        return text[:idx+1].strip()\n",
    "    else:\n",
    "        return text.strip()\n",
    "\n",
    "gen_ans = [cut_to_last_period(ans) for ans in gen_ans]\n",
    "\n",
    "gen_embs = medcpt_embed(gen_ans)\n",
    "ref_embs = medcpt_embed(ref_ans)\n",
    "\n",
    "num   = (gen_embs * ref_embs).sum(axis=1)\n",
    "denom = np.linalg.norm(gen_embs, axis=1) * np.linalg.norm(ref_embs, axis=1)\n",
    "sim   = num / denom\n",
    "\n",
    "bleu_refs = [[ref.split()] for ref in ref_ans]\n",
    "bleu_hyps = [gen.split() for gen in gen_ans]\n",
    "bleu_score = corpus_bleu(bleu_refs, bleu_hyps, smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "rouge_l_f1s = []\n",
    "for ref, gen in zip(ref_ans, gen_ans):\n",
    "    scores = rouge.score(ref, gen)\n",
    "    rouge_l_f1s.append(scores['rougeL'].fmeasure)\n",
    "rouge_l_f1 = sum(rouge_l_f1s) / len(rouge_l_f1s) if rouge_l_f1s else 0.0\n",
    "\n",
    "print(f\"\\nTest set mean cosine similarity: {np.mean(sim):.4f}\")\n",
    "print(f\"Test set BLEU: {bleu_score:.4f}\")\n",
    "print(f\"Test set ROUGE-LCS F1: {rouge_l_f1:.4f}\")\n",
    "\n",
    "for i in range(min(3, len(qs))):\n",
    "    print(f\"\\nQ: {qs[i]}\\nReference: {ref_ans[i]}\\nGenerated: {gen_ans[i]}\\nCosine similarity: {sim[i]:.4f}\") \n",
    "\n",
    "'''\n",
    "<Example >\n",
    "Reference: (1) Antibiotics are recommended to be maintained as they are, and adjusted according to the results of culture. (2) Esophageal candidasis is also suspected, and fluconazole dosage is recommended to be changed to 400 mg on day 1, then 200 to 400 mg once daily as a normal new functional baseline. Currently, after application of PTGBD, a reduction in inflammatory markers and a fever spike is not observed, but a suppressed RUQ tenderness is observed.\n",
    "Generated: : #. There is no specific recommendation for empirical antibiotic therapy because there is limited information about the previous medical condition and current status of the patient who has already recovered from the asymptomatic stage of the past infection until now, but only empirically administered vancomycin and meropenem were given up to the time of confirmation of the BAL culture result, so please continue administration of vanco + mero according to the doctor's discretion until the recovery of the lung function is completed. If you want to change the medication, we recommend confirming the susceptibility test of the cultured bacteria before changing the drug.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "240305",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
